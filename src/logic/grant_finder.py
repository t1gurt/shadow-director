import re
import logging
import asyncio
from typing import List, Dict, Any, Optional
from google.genai.types import GenerateContentConfig, ThinkingConfig
from src.tools.search_tool import SearchTool
from src.logic.grant_validator import GrantValidator
from src.logic.grant_page_scraper import GrantPageScraper
from src.utils.progress_notifier import get_progress_notifier, ProgressStage

class GrantFinder:
    """
    Handles grant search operations including query generation and official page lookup.
    Uses Playwright-based GrantPageScraper for enhanced page verification.
    Implements SGNA (Search-Ground-Navigate-Act) model for improved accuracy.
    """
    
    # Trusted domains for grant information (SGNA model: Site Restrictions)
    TRUSTED_DOMAINS = [
        'go.jp',      # æ”¿åºœæ©Ÿé–¢
        'or.jp',      # è²¡å›£æ³•äººãƒ»NPO
        'lg.jp',      # åœ°æ–¹è‡ªæ²»ä½“
        'ac.jp',      # å­¦è¡“æ©Ÿé–¢
        'org',        # éå–¶åˆ©çµ„ç¹”
        'co.jp',      # ä¼æ¥­ï¼ˆCSRåŠ©æˆé‡‘ï¼‰
        'com',        # å›½éš›ä¼æ¥­
    ]
    
    def __init__(self, client, model_name: str, config: Dict[str, Any]):
        self.client = client
        self.model_name = model_name
        self.config = config
        self.search_tool = SearchTool()
        self.validator = GrantValidator()
        self.page_scraper = GrantPageScraper()
        self.system_prompt = self.config.get("system_prompts", {}).get("observer", "")

    def generate_queries(self, profile: str) -> List[str]:
        """
        Generates optimized search queries based on the Soul Profile.
        """
        # Get prompt template from config
        prompt_template = self.config.get("system_prompts", {}).get("observer_query_generator", "")
        if prompt_template:
            prompt = prompt_template.format(profile=profile)
        else:
            # Fallback to inline prompt if template not found
            prompt = f"""
ç¾åœ¨ Soul Profile:
{profile}

ã‚¿ã‚¹ã‚¯:
ã“ã®NPOã«æœ€é©ãªè³‡é‡‘èª¿é”æ©Ÿä¼šï¼ˆåŠ©æˆé‡‘ã€CSRï¼‰ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã®3ã¤ã®ç•°ãªã‚‹æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ãƒŸãƒƒã‚·ãƒ§ãƒ³ã€å¯¾è±¡èª²é¡Œã€ç‹¬è‡ªã®å¼·ã¿ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚
ã‚¯ã‚¨ãƒªã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚1è¡Œã«1ã¤ã®ã‚¯ã‚¨ãƒªã€‚
"""
        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt
            )
            queries = [q.strip() for q in response.text.strip().split('\n') if q.strip()]
            return queries[:3] # Limit to top 3
        except Exception as e:
            logging.error(f"Error generating queries: {e}")
            return [f"NPOåŠ©æˆé‡‘ {profile[:50]}..."] # Fallback

    def parse_opportunities(self, text: str) -> List[Dict]:
        """
        Parse structured opportunity data from Observer response.
        """
        opportunities = []
        
        # Validate text parameter
        if not text or not isinstance(text, str):
            logging.warning("[GRANT_FINDER] parse_opportunities received invalid text parameter")
            return opportunities
        
        # Split by ### æ©Ÿä¼š pattern
        sections = re.split(r'###\s*æ©Ÿä¼š\s*\d+:', text)
        
        for section in sections[1:]:  # Skip first empty section
            try:
                # Extract title (first line)
                lines = section.strip().split('\n')
                title = lines[0].strip() if lines else "ä¸æ˜"
                
                # Extract URL
                url_match = re.search(r'\*\*URL\*\*:\s*(.+)', section)
                url = url_match.group(1).strip() if url_match else "N/A"
                
                # Extract amount
                amount_match = re.search(r'\*\*é‡‘é¡\*\*:\s*(.+)', section)
                amount = amount_match.group(1).strip() if amount_match else "N/A"
                
                # Extract resonance score
                score_match = re.search(r'\*\*å…±é³´ã‚¹ã‚³ã‚¢\*\*:\s*(\d+)', section)
                score = int(score_match.group(1)) if score_match else 0
                
                # Extract reason
                reason_match = re.search(r'\*\*å…±é³´ç†ç”±\*\*:\s*(.+)', section)
                reason = reason_match.group(1).strip() if reason_match else "ç†ç”±ä¸æ˜"
                
                opportunities.append({
                    "title": title,
                    "url": url,
                    "amount": amount,
                    "resonance_score": score,
                    "reason": reason
                })
                
                logging.debug(f"[DEBUG] Parsed opportunity: {title} (Score: {score})")
            except Exception as e:
                logging.error(f"[ERROR] Failed to parse opportunity section: {e}")
                continue
        
        return opportunities

    def search_grants(self, profile: str, current_date: str, excluded_grants: str = None) -> tuple[str, List[Dict]]:
        """
        Executes first step of observation: Generates queries and searches for grants.
        Returns the raw response text and parsed opportunities.
        """
        queries = self.generate_queries(profile)
        logging.info(f"Generated Search Queries: {queries}")
        
        # Get prompt template from config
        prompt_template = self.config.get("system_prompts", {}).get("observer_search_task", "")
        
        full_prompt = ""
        if prompt_template:
            if "{current_date}" in prompt_template:
                full_prompt = prompt_template.format(
                    system_prompt=self.system_prompt,
                    profile=profile,
                    queries=', '.join(queries),
                    current_date=current_date,
                    excluded_grants=excluded_grants or "ãªã—"
                )
            else:
                full_prompt = prompt_template.format(
                    system_prompt=self.system_prompt,
                    profile=profile,
                    queries=', '.join(queries)
                )
                full_prompt += f"\n\n**é‡è¦**: æœ¬æ—¥ã¯{current_date}ã§ã™ã€‚ç¾åœ¨å‹Ÿé›†ä¸­ã®åŠ©æˆé‡‘ã®ã¿ã‚’å ±å‘Šã—ã¦ãã ã•ã„ã€‚"
                if excluded_grants:
                    full_prompt += f"\n\n**é™¤å¤–ãƒªã‚¹ãƒˆï¼ˆãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆæ¸ˆã¿ï¼‰**: {excluded_grants}"
        else:
             full_prompt = f"""
{self.system_prompt}

ç¾åœ¨ã®Soul Profile:
{profile}

æ¤œç´¢æˆ¦ç•¥:
ä»¥ä¸‹ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¾ã—ãŸ:
{', '.join(queries)}

ã‚¿ã‚¹ã‚¯:
æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ã“ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã¨å…±é³´ã™ã‚‹ç¾åœ¨ã®NPOåŠ©æˆé‡‘ã‚„CSRè³‡é‡‘èª¿é”æ©Ÿä¼šã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚
ã‚¯ã‚¨ãƒªãŒç¤ºå”†ã™ã‚‹æˆ¦ç•¥ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
è¦‹ã¤ã‹ã£ãŸä¸Šä½3ã¤ã®æ©Ÿä¼šã«ã¤ã„ã¦å ±å‘Šã—ã¦ãã ã•ã„ã€‚
"""

        try:
            # Enable Google Search Tool
            tool_config = self.search_tool.get_tool_config()
            
            # Gemini 3.0 Thinking Mode for grant discovery
            thinking_config = ThinkingConfig(thinking_level="high")
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=full_prompt,
                config=GenerateContentConfig(
                    tools=[tool_config],
                    thinking_config=thinking_config
                )
            )
            response_text = response.text if response.text else ""
            
            # Validate response_text before parsing
            if not response_text:
                logging.warning("[GRANT_FINDER] Empty response from Gemini API")
                return "æ¤œç´¢çµæœãŒç©ºã§ã—ãŸ", []
            
            # Here we could extract grounding metadata as before if needed, 
            # but for now we focus on the text response parsing.
            
            opportunities = self.parse_opportunities(response_text)
            return response_text, opportunities
            
        except Exception as e:
            logging.error(f"Error in search_grants: {e}")
            return f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}", []

    def find_official_page(self, grant_name: str, current_date: str) -> Dict:
        """
        Searches for the official grant page and verifies the application deadline.
        Uses organization name + targeted keywords for better search accuracy.
        """
        logging.info(f"[GRANT_FINDER] Finding official page for: {grant_name}")
        
        # Create shortened grant name for display (max 20 chars)
        grant_display_name = grant_name[:20] + "..." if len(grant_name) > 20 else grant_name
        
        result = {
            'official_url': 'N/A',
            'domain': '',
            'deadline_start': '',
            'deadline_end': '',
            'status': 'ä¸æ˜',
            'is_valid': False,
            'confidence': 'ä½',
            'confidence_reason': ''
        }
        
        # Extract organization name for targeted search
        org_name = self.validator.extract_organization_name(grant_name)
        if org_name:
            logging.info(f"[GRANT_FINDER] Extracted org name: {org_name}")
        
        # Build improved search prompt
        prompt_template = self.config.get("system_prompts", {}).get("observer_find_official_page", "")
        
        # Extract current year from current_date for search optimization
        current_year = "2026"
        if current_date:
            year_match = re.search(r'(\d{4})', current_date)
            if year_match:
                current_year = year_match.group(1)
        
        # Build site restriction string for trusted domains (SGNA model)
        site_restriction = " OR ".join([f"site:{d}" for d in self.TRUSTED_DOMAINS])
        
        # Create a more targeted search query with SGNA model enhancements
        if org_name:
            search_hint = f"""
**æ¤œç´¢æˆ¦ç•¥ï¼ˆSGNAãƒ¢ãƒ‡ãƒ« - é‡è¦ï¼‰:**

**Step 1: ä¿¡é ¼ã§ãã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰æ¤œç´¢**
æ¤œç´¢ã‚¯ã‚¨ãƒªã«ä»¥ä¸‹ã®ã‚µã‚¤ãƒˆåˆ¶é™ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
`"{org_name} åŠ©æˆé‡‘ å‹Ÿé›† {current_year}" ({site_restriction})`

**Step 2: ç€é™¸ãƒšãƒ¼ã‚¸å„ªå…ˆ**
- PDFã¸ã®ç›´æ¥ãƒªãƒ³ã‚¯ã§ã¯ãªãã€HTMLã®ã€Œå…¬å‹Ÿè¦é ˜ãƒšãƒ¼ã‚¸ã€ã‚’æ¢ã—ã¦ãã ã•ã„
- ç›´ãƒªãƒ³ã‚¯ã¯ãƒªãƒ³ã‚¯åˆ‡ã‚Œãƒªã‚¹ã‚¯ãŒé«˜ãã€æœ€æ–°ç‰ˆã‹ã©ã†ã‹ã®åˆ¤æ–­ãŒå›°é›£ã§ã™

**Step 3: æœ€æ–°æƒ…å ±ã®ç¢ºèª**
- ã€Œ{current_year}å¹´åº¦ã€ã€Œç¬¬â—‹å›ã€ã€Œä»¤å’Œâ—‹å¹´ã€ãªã©ã®è¡¨è¨˜ã‚’ç¢ºèª
- å¤ã„å¹´åº¦ã®ãƒšãƒ¼ã‚¸ã‚’é¿ã‘ã¦ãã ã•ã„

**æ³¨æ„:** åŠ©æˆé‡‘åã€Œ{grant_name}ã€ã§ç›´æ¥æ¤œç´¢ã™ã‚‹ã¨å¤ã„ãƒšãƒ¼ã‚¸ãŒãƒ’ãƒƒãƒˆã—ã‚„ã™ã„ãŸã‚ã€
ã¾ãšçµ„ç¹”ã®åŠ©æˆé‡‘ãƒãƒ¼ã‚¿ãƒ«ãƒšãƒ¼ã‚¸ã‚’è¦‹ã¤ã‘ã€ãã“ã‹ã‚‰è©²å½“ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚
"""
        else:
            search_hint = f"""
**æ¤œç´¢æˆ¦ç•¥ï¼ˆSGNAãƒ¢ãƒ‡ãƒ«ï¼‰:**
åŠ©æˆé‡‘ã€Œ{grant_name}ã€ã®å…¬å¼ãƒšãƒ¼ã‚¸ã‚’ä»¥ä¸‹ã®æ¡ä»¶ã§æ¤œç´¢ã—ã¦ãã ã•ã„ï¼š
- ä¿¡é ¼ã§ãã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³: {site_restriction}
- å¹´åº¦: {current_year}å¹´åº¦ã¾ãŸã¯æœ€æ–°ã®å…¬å‹Ÿ
- HTMLãƒšãƒ¼ã‚¸ã‚’å„ªå…ˆï¼ˆPDFã¸ã®ç›´ãƒªãƒ³ã‚¯ã‚ˆã‚Šç€é™¸ãƒšãƒ¼ã‚¸ã‚’å„ªå…ˆï¼‰
"""
        
        if prompt_template:
            full_prompt = prompt_template.format(
                grant_name=grant_name,
                current_date=current_date
            )
            # Append search strategy hint
            full_prompt = search_hint + "\n" + full_prompt
        else:
            full_prompt = f"""
{search_hint}

åŠ©æˆé‡‘ã€Œ{grant_name}ã€ã®å…¬å¼ç”³è«‹ãƒšãƒ¼ã‚¸ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚

æœ¬æ—¥: {current_date}

**å‡ºåŠ›å½¢å¼:**
- **å…¬å¼URL**: [æ­£ç¢ºãªURL]
- **ãƒ‰ãƒ¡ã‚¤ãƒ³**: [ãƒ‰ãƒ¡ã‚¤ãƒ³å]
- **å‹Ÿé›†é–‹å§‹æ—¥**: [æ—¥ä»˜]
- **å‹Ÿé›†çµ‚äº†æ—¥**: [æ—¥ä»˜]
- **å‹Ÿé›†çŠ¶æ³**: [å‹Ÿé›†ä¸­/å‹Ÿé›†çµ‚äº†/ä»Šå¾Œå‹Ÿé›†äºˆå®š/ä¸æ˜]
- **ä¿¡é ¼åº¦**: [é«˜/ä¸­/ä½]
- **ä¿¡é ¼åº¦ç†ç”±**: [ç†ç”±]
"""
        
        try:
            tool_config = self.search_tool.get_tool_config()
            
            # Gemini 3.0 Thinking Mode for deep reasoning during page investigation
            thinking_config = ThinkingConfig(thinking_level="high")
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=full_prompt,
                config=GenerateContentConfig(
                    tools=[tool_config],
                    temperature=0.2,
                    thinking_config=thinking_config
                )
            )
            
            response_text = response.text
            logging.info(f"[GRANT_FINDER] Response: {response_text[:200]}...")
            
            # Parse response
            url_match = re.search(r'\*\*å…¬å¼URL\*\*:\s*(.+)', response_text)
            if url_match:
                url = url_match.group(1).strip()
                result['official_url'] = self.validator.resolve_redirect_url(url)
            
            domain_match = re.search(r'\*\*ãƒ‰ãƒ¡ã‚¤ãƒ³\*\*:\s*(.+)', response_text)
            if domain_match:
                result['domain'] = domain_match.group(1).strip()
            
            start_match = re.search(r'\*\*å‹Ÿé›†é–‹å§‹æ—¥\*\*:\s*(.+)', response_text)
            if start_match:
                result['deadline_start'] = start_match.group(1).strip()
            
            end_match = re.search(r'\*\*å‹Ÿé›†çµ‚äº†æ—¥\*\*:\s*(.+)', response_text)
            if end_match:
                result['deadline_end'] = end_match.group(1).strip()
            
            status_match = re.search(r'\*\*å‹Ÿé›†çŠ¶æ³\*\*:\s*(.+)', response_text)
            if status_match:
                status = status_match.group(1).strip()
                result['status'] = status
                if 'å‹Ÿé›†ä¸­' in status or 'ä»Šå¾Œ' in status or 'äºˆå®š' in status:
                    result['is_valid'] = True
                elif 'çµ‚äº†' in status or 'ç· åˆ‡' in status:
                    result['is_valid'] = False
            
            confidence_match = re.search(r'\*\*ä¿¡é ¼åº¦\*\*:\s*(.+)', response_text)
            if confidence_match:
                result['confidence'] = confidence_match.group(1).strip()
            
            reason_match = re.search(r'\*\*ä¿¡é ¼åº¦ç†ç”±\*\*:\s*(.+)', response_text)
            if reason_match:
                result['confidence_reason'] = reason_match.group(1).strip()
            
            # Validation Step
            if result['official_url'] != 'N/A':
                notifier = get_progress_notifier()
                
                quality_score, quality_reason = self.validator.evaluate_url_quality(result['official_url'], grant_name)
                result['url_quality_score'] = quality_score
                result['url_quality_reason'] = quality_reason
                
                # Agent Thought: åˆ¤æ–­æ ¹æ‹ ã‚’å…ˆã«è¡¨ç¤ºï¼ˆè„³å†…é–‹ç¤ºï¼‰
                notifier.notify_thought(
                    f"[{grant_display_name}] ãƒ‰ãƒ¡ã‚¤ãƒ³è§£æå®Œäº†",
                    quality_reason
                )
                
                # Notify user about URL quality with enhanced format
                if quality_score >= 70:
                    notifier.notify_sync(ProgressStage.VERIFYING, f"[{grant_display_name}] â¡ ä¿¡é ¼æ€§è©•ä¾¡: {quality_score}ç‚¹ (Verified)", None)
                elif quality_score >= 50:
                    notifier.notify_sync(ProgressStage.ANALYZING, f"[{grant_display_name}] â¡ ä¿¡é ¼æ€§è©•ä¾¡: {quality_score}ç‚¹", None)
                else:
                    notifier.notify_sync(ProgressStage.WARNING, f"[{grant_display_name}] â¡ ä¿¡é ¼æ€§è©•ä¾¡: {quality_score}ç‚¹ï¼ˆä½ï¼‰", None)
                
                if quality_score < 50:
                    logging.warning(f"[GRANT_FINDER] Low quality URL: {result['official_url']}")
                    result['is_valid'] = False
                
                is_accessible, access_status, final_url = self.validator.validate_url_accessible(result['official_url'])
                result['url_accessible'] = is_accessible
                result['url_access_status'] = access_status
                
                # Notify user about accessibility
                if is_accessible:
                    notifier.notify_sync(ProgressStage.ANALYZING, f"[{grant_display_name}] âœ… å…¬å¼ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½", f"URL: {final_url[:60]}...")
                else:
                    notifier.notify_sync(ProgressStage.WARNING, f"[{grant_display_name}] âŒ å…¬å¼ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯", access_status)
                
                if is_accessible and final_url:
                    result['official_url'] = final_url
                    
                    # Enhanced verification with Playwright
                    try:
                        logging.info(f"[GRANT_FINDER] Running Playwright verification for: {final_url}")
                        notifier.notify_sync(ProgressStage.ANALYZING, f"[{grant_display_name}] ğŸ” Playwrightã§è©³ç´°æ¤œè¨¼ä¸­...", "ãƒšãƒ¼ã‚¸å†…å®¹ã‚’è§£æã—ã¦ã„ã¾ã™")
                        
                        playwright_result = self._run_playwright_verification(final_url, grant_name)
                        
                        if playwright_result:
                            # éšœå®³æ¤œçŸ¥ã®ç¢ºèªï¼ˆãƒ­ã‚°ã‚¤ãƒ³å£ã€404ç­‰ï¼‰
                            if playwright_result.get('obstacle_detected'):
                                obstacle_type = playwright_result.get('obstacle_type', 'ä¸æ˜ãªéšœå®³')
                                page_title = playwright_result.get('title', '')
                                
                                # éšœå®³æ¤œçŸ¥ã‚’è¡¨ç¤º
                                notifier.notify_obstacle(obstacle_type, f"ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: \"{page_title}\"")
                                
                                # Agent Thought: éšœå®³ã¸ã®å¯¾å¿œã‚’èª¬æ˜
                                notifier.notify_thought(
                                    f"[{grant_display_name}] éšœå®³ã‚’æ¤œå‡º",
                                    f"ã“ã®URLã¯{obstacle_type}ã§ã™ã€‚å…¬å‹Ÿæƒ…å ±ã‚’å–å¾—ã§ããªã„ãŸã‚ã€ä»£æ›¿ãƒ«ãƒ¼ãƒˆã‚’æ¢ç´¢ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
                                )
                                
                                logging.warning(f"[GRANT_FINDER] Obstacle detected: {obstacle_type}")
                            else:
                                result['playwright_verified'] = True
                                result['playwright_confidence'] = playwright_result.get('confidence', 0)
                                result['format_files'] = playwright_result.get('format_files', [])
                                
                                # Notify Playwright results
                                file_count = len(result.get('format_files', []))
                                if file_count > 0:
                                    notifier.notify_sync(ProgressStage.ANALYZING, f"[{grant_display_name}] ğŸ“ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ« {file_count}ä»¶ ç™ºè¦‹", "ç”³è«‹æ›¸æ§˜å¼ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
                                
                                # Update deadline info if found
                                if playwright_result.get('deadline_info'):
                                    deadline = playwright_result['deadline_info']
                                    if deadline.get('date'):
                                        result['deadline_end'] = deadline['date']
                                        notifier.notify_sync(ProgressStage.ANALYZING, f"[{grant_display_name}] ğŸ“… ç· åˆ‡æ—¥: {deadline['date']}", "ãƒšãƒ¼ã‚¸ã‹ã‚‰ç· åˆ‡æ—¥ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
                                
                                logging.info(f"[GRANT_FINDER] Playwright found {file_count} format files")
                        else:
                            notifier.notify_sync(ProgressStage.ANALYZING, f"[{grant_display_name}] â„¹ï¸ Playwrightæ¤œè¨¼å®Œäº†", "è¿½åŠ æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    except Exception as pw_error:
                        logging.warning(f"[GRANT_FINDER] Playwright verification failed: {pw_error}")
                        result['playwright_verified'] = False
                else:
                    # Retry logic
                    return self._retry_find_official_page(grant_name, result, access_status)

            logging.info(f"[GRANT_FINDER] Result: URL={result['official_url'][:50]}..., Valid={result['is_valid']}")
            
        except Exception as e:
            logging.error(f"[GRANT_FINDER] Error finding official page: {e}")
        
        return result
    
    def _run_playwright_verification(self, url: str, grant_name: str) -> Optional[Dict[str, Any]]:
        """
        Run Playwright-based page verification.
        Uses run_sync to safe execution.
        """
        try:
            from src.tools.site_explorer import run_sync
            return run_sync(self._async_playwright_verification(url, grant_name))
        except Exception as e:
            logging.error(f"[GRANT_FINDER] Playwright verification error: {e}")
            return None
    
    async def _async_playwright_verification(self, url: str, grant_name: str) -> Optional[Dict[str, Any]]:
        """
        Async Playwright verification.
        """
        try:
            grant_info = await self.page_scraper.find_grant_info(url, grant_name)
            
            if not grant_info.get('accessible'):
                return None
            
            return {
                'verified': True,
                'confidence': 80 if grant_info.get('format_files') else 50,
                'format_files': grant_info.get('format_files', []),
                'deadline_info': grant_info.get('deadline_info'),
                'related_links': grant_info.get('related_links', [])
            }
        except Exception as e:
            logging.error(f"[GRANT_FINDER] Async Playwright error: {e}")
            return None

    def _sanitize_grant_name(self, grant_name: str) -> str:
        """
        åŠ©æˆé‡‘åã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒãƒ³ãƒ‰ï¼ˆã€Œãƒ‰ãƒ©ãƒ•ãƒˆã‚’ä½œæˆã—ã¦ã€ç­‰ï¼‰ã‚’é™¤å»ã™ã‚‹ã€‚
        
        Args:
            grant_name: ã‚µãƒ‹ã‚¿ã‚¤ã‚ºå¯¾è±¡ã®åŠ©æˆé‡‘å
            
        Returns:
            ã‚µãƒ‹ã‚¿ã‚¤ã‚ºæ¸ˆã¿ã®åŠ©æˆé‡‘å
        """
        if not grant_name:
            return ""
        
        # é™¤å»ã™ã¹ããƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆã‚³ãƒãƒ³ãƒ‰ç³»ï¼‰
        remove_phrases = [
            'ã®ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ä½œæˆã—ã¦',
            'ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ä½œæˆã—ã¦',
            'ã®ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆ',
            'ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆ',
            'ã®ç”³è«‹æ›¸ã‚’æ›¸ã„ã¦',
            'ç”³è«‹æ›¸ã‚’æ›¸ã„ã¦',
            'ã‚’æ›¸ã„ã¦',
            'ã«ã¤ã„ã¦èª¿ã¹ã¦',
            'ã«ã¤ã„ã¦è©³ã—ã',
            'ã‚’èª¿ã¹ã¦',
            'ã®è©³ç´°',
            'ã‚’æ¢ã—ã¦',
            'ã®ãƒ‰ãƒ©ãƒ•ãƒˆã‚’æ¢ã—ã¦',
        ]
        
        sanitized = grant_name
        for phrase in remove_phrases:
            sanitized = sanitized.replace(phrase, '')
        
        return sanitized.strip()

    def _extract_grant_keywords(self, grant_name: str) -> str:
        """
        Extract meaningful keywords from grant name, excluding generic organizational terms.
        Returns space-separated keywords suitable for search queries.
        """
        if not grant_name:
            return ""
        
        # Remove organizational prefixes
        cleaned = grant_name
        prefixes_to_remove = [
            'å…¬ç›Šè²¡å›£æ³•äºº', 'ä¸€èˆ¬è²¡å›£æ³•äºº', 'å…¬ç›Šç¤¾å›£æ³•äºº', 'ä¸€èˆ¬ç¤¾å›£æ³•äºº',
            'ç¤¾ä¼šç¦ç¥‰æ³•äºº', 'ç‰¹å®šéå–¶åˆ©æ´»å‹•æ³•äºº', 'NPOæ³•äºº',
            'ç‹¬ç«‹è¡Œæ”¿æ³•äºº', 'åœ°æ–¹ç‹¬ç«‹è¡Œæ”¿æ³•äºº', 'å›½ç«‹ç ”ç©¶é–‹ç™ºæ³•äºº',
            'ä»¤å’Œ', 'å¹³æˆ', 'å¹´åº¦', 'ç¬¬', 'å›'
        ]
        
        for prefix in prefixes_to_remove:
            cleaned = cleaned.replace(prefix, ' ')
        
        # Remove generic terms
        generic_terms = [
            'åŠ©æˆé‡‘', 'è£œåŠ©é‡‘', 'æ”¯æ´é‡‘', 'äº¤ä»˜é‡‘', 'å…¬å‹Ÿ', 'å‹Ÿé›†',
            'ç”³è«‹', 'å¿œå‹Ÿ', 'ãƒ—ãƒ­ã‚°ãƒ©ãƒ ', 'äº‹æ¥­', 'åˆ¶åº¦'
        ]
        
        for term in generic_terms:
            cleaned = cleaned.replace(term, ' ')
        
        # Extract meaningful words (2+ characters)
        import re
        words = re.findall(r'[ä¸€-é¾¯ã‚¡-ãƒ¶ãƒ¼\w]{2,}', cleaned)
        
        # Filter out numbers and year patterns
        meaningful_words = []
        for word in words:
            # Skip if it's just numbers
            if re.match(r'^\d+$', word):
                continue
            # Skip year patterns like 2026
            if re.match(r'^20\d{2}$', word):
                continue
            meaningful_words.append(word)
        
        # Take first 2-3 meaningful words
        keywords = ' '.join(meaningful_words[:3])
        
        return keywords.strip()

    def _retry_find_official_page(self, grant_name: str, previous_result: Dict, failure_reason: str) -> Dict:
        """
        Retries finding the official page if the first attempt failed validation.
        Enhanced with:
        1. Multiple search query variations
        2. Playwright-based site exploration
        3. Up to 3 retry attempts
        """
        logging.info(f"[GRANT_FINDER] Retrying for: {grant_name}")
        notifier = get_progress_notifier()
        
        # Create shortened grant name for display (max 20 chars)
        grant_display_name = grant_name[:20] + "..." if len(grant_name) > 20 else grant_name
        
        # éšœå®³æ¤œçŸ¥ã‚’è¡¨ç¤ºï¼ˆãƒªã‚«ãƒãƒªãƒ¼æ¼”å‡ºï¼‰
        notifier.notify_obstacle("ã‚¢ã‚¯ã‚»ã‚¹ä¸èƒ½", f"[{grant_display_name}] {failure_reason}")
        
        # Agent Thought: æˆ¦ç•¥å¤‰æ›´ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’è¡¨ç¤º
        notifier.notify_thought(
            f"[{grant_display_name}] æˆ¦ç•¥å¤‰æ›´",
            f"æŒ‡å®šã•ã‚ŒãŸURLã«ã‚¢ã‚¯ã‚»ã‚¹ã§ããªã„ãŸã‚ã€åŠ©æˆé‡‘åã‚’ã‚­ãƒ¼ã«ä¸€èˆ¬å…¬é–‹ã•ã‚Œã¦ã„ã‚‹å…¬å¼ãƒšãƒ¼ã‚¸ã‚’Googleæ¤œç´¢ã§æ¢ã—ã¾ã™ã€‚"
        )
        
        # ãƒªã‚«ãƒãƒªãƒ¼æ¼”å‡º: å†æ¤œç´¢é–‹å§‹
        notifier.notify_recovery(f"[{grant_display_name}] å†æ¤œç´¢ã‚’å®Ÿè¡Œä¸­...", "ä»£æ›¿URLã‚’æ¢ç´¢")
        
        # Extract organization name for targeted retry search
        org_name = self.validator.extract_organization_name(grant_name)
        
        # ã‚µãƒ‹ã‚¿ã‚¤ã‚ºæ¸ˆã¿ã®åŠ©æˆé‡‘åã‚’å–å¾—ï¼ˆã‚³ãƒãƒ³ãƒ‰ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’é™¤å»ï¼‰
        sanitized_grant_name = self._sanitize_grant_name(grant_name)
        
        # Extract key terms from grant name (exclude generic terms)
        grant_keywords = self._extract_grant_keywords(grant_name)
        
        logging.info(f"[GRANT_FINDER] æ¤œç´¢æˆ¦ç•¥: å…¨ä½“å='{sanitized_grant_name}', ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰={grant_keywords}")
        
        # Validate: skip generic organization names
        if org_name:
            generic_org_names = ['å…¬ç›Šè²¡å›£', 'ä¸€èˆ¬è²¡å›£', 'å…¬ç›Šç¤¾å›£', 'ä¸€èˆ¬ç¤¾å›£', 'ç¤¾ä¼šç¦ç¥‰æ³•äºº', 'å…¬ç›Š', 'ä¸€èˆ¬']
            if org_name in generic_org_names:
                logging.warning(f"[GRANT_FINDER] Extracted org_name is too generic: {org_name}, using grant_name instead")
                org_name = None
        
        # æ¤œç´¢æˆ¦ç•¥ã‚’æ”¹å–„ï¼šã¾ãšåŠ©æˆé‡‘åå…¨ä½“ã§æ¤œç´¢ã€æ¬¡ã«ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        search_queries = []
        
        # Strategy 1: ã¾ãšåŠ©æˆé‡‘åå…¨ä½“ã§æ¤œç´¢ï¼ˆæœ€å„ªå…ˆï¼‰
        if sanitized_grant_name and len(sanitized_grant_name) > 5:
            search_queries.append(f'"{sanitized_grant_name}" å…¬å¼')
            search_queries.append(f'"{sanitized_grant_name}" å‹Ÿé›†')
        
        # Strategy 2: çµ„ç¹”å + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®çµ„ã¿åˆã‚ã›ï¼ˆä»£æ›¿ï¼‰
        if org_name and grant_keywords:
            search_queries.extend([
                f"{org_name} {grant_keywords} å‹Ÿé›† 2026",
                f"{org_name} {grant_keywords} ç”³è«‹",
            ])
        elif org_name:
            search_queries.extend([
                f"{org_name} åŠ©æˆé‡‘ å‹Ÿé›† 2026",
                f"{org_name} è£œåŠ©é‡‘ ç”³è«‹",
            ])
        
        # Strategy 3: ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ã¿ï¼ˆæœ€å¾Œã®æ‰‹æ®µï¼‰
        if grant_keywords and len(search_queries) < 3:
            search_queries.append(f"{grant_keywords} åŠ©æˆé‡‘ å…¬å¼")
        
        # Try up to 3 different search strategies
        max_retries = min(3, len(search_queries))
        
        for retry_num in range(max_retries):
            query = search_queries[retry_num]
            notifier.notify_sync(ProgressStage.SEARCHING, f"[{grant_display_name}] ğŸ” ä»£æ›¿æ¤œç´¢ ({retry_num + 1}/{max_retries})", f"æ¤œç´¢: {query[:40]}...")
            logging.info(f"[GRANT_FINDER] Retry {retry_num + 1}: searching with '{query}'")
            
            # Build site restriction for retry (SGNA model)
            site_restriction = " OR ".join([f"site:{d}" for d in self.TRUSTED_DOMAINS])
            
            retry_prompt = f"""
åŠ©æˆé‡‘ã®å…¬å¼ç”³è«‹ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚

**æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆSGNAãƒ¢ãƒ‡ãƒ«ï¼‰:** `"{query}" ({site_restriction})`

**æ¢ã—ã¦ã„ã‚‹åŠ©æˆé‡‘:** {grant_name}

**é‡è¦æ¡ä»¶:**
1. ä¿¡é ¼ã§ãã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿: go.jp, or.jp, lg.jp, co.jp, org, com
2. **ç€é™¸ãƒšãƒ¼ã‚¸å„ªå…ˆ**: PDFã¸ã®ç›´ãƒªãƒ³ã‚¯ã§ã¯ãªãã€HTMLã®å…¬å‹Ÿãƒšãƒ¼ã‚¸ã‚’é¸æŠ
3. æœ€æ–°ã®å…¬å‹Ÿæƒ…å ±ã§ã‚ã‚‹ã“ã¨ï¼ˆå¹´åº¦ã‚’ç¢ºèªï¼‰

**å‡ºåŠ›å½¢å¼:**
- **å…¬å¼URL**: [æ­£ç¢ºãªURL]
"""
            try:
                # Gemini 3.0 Thinking Mode for retry search
                thinking_config = ThinkingConfig(thinking_level="high")
                
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=retry_prompt,
                    config=GenerateContentConfig(
                        tools=[self.search_tool.get_tool_config()],
                        temperature=0.1,
                        thinking_config=thinking_config
                    )
                )

                logging.info(f"[GRANT_FINDER] Retry {retry_num + 1} response: {response.text}")
                
                retry_url_match = re.search(r'\*\*å…¬å¼URL\*\*:\s*(.+)', response.text)
                if retry_url_match:
                    retry_url = retry_url_match.group(1).strip()
                    retry_url = self.validator.resolve_redirect_url(retry_url)
                    
                    # Skip if same as failed URL
                    if retry_url == previous_result.get('official_url'):
                        logging.info(f"[GRANT_FINDER] Same URL found, trying next query")
                        continue
                    
                    is_retry_accessible, retry_status, retry_final_url = self.validator.validate_url_accessible(retry_url)
                    
                    if is_retry_accessible and retry_final_url:
                        notifier.notify_sync(ProgressStage.ANALYZING, f"[{grant_display_name}] âœ… ä»£æ›¿URLç™ºè¦‹ (è©¦è¡Œ{retry_num + 1})", retry_final_url[:60])
                        
                        previous_result['official_url'] = retry_final_url
                        previous_result['url_accessible'] = True
                        previous_result['url_access_status'] = f"ãƒªãƒˆãƒ©ã‚¤æˆåŠŸï¼ˆè©¦è¡Œ{retry_num + 1}ï¼‰"
                        logging.info(f"[GRANT_FINDER] Retry {retry_num + 1} successful: {retry_final_url}")
                        return previous_result
                    else:
                        logging.info(f"[GRANT_FINDER] Retry {retry_num + 1} URL not accessible: {retry_status}")
                        
            except Exception as retry_e:
                logging.error(f"[GRANT_FINDER] Retry {retry_num + 1} error: {retry_e}")
        
        # All LLM retries failed - try Playwright exploration as last resort
        if org_name:
            notifier.notify_sync(ProgressStage.SEARCHING, f"[{grant_display_name}] ğŸ” Playwrightæ·±æ˜ã‚Šæ¤œç´¢ä¸­...", f"çµ„ç¹”ã‚µã‚¤ãƒˆã‚’æ¢ç´¢: {org_name}")
            playwright_url = self._playwright_find_grant_page(org_name, grant_name)
            
            if playwright_url:
                is_accessible, status, final_url = self.validator.validate_url_accessible(playwright_url)
                if is_accessible and final_url:
                    notifier.notify_sync(ProgressStage.ANALYZING, f"[{grant_display_name}] âœ… Playwrightã§ä»£æ›¿URLç™ºè¦‹", final_url[:60])
                    previous_result['official_url'] = final_url
                    previous_result['url_accessible'] = True
                    previous_result['url_access_status'] = "Playwrightæ¤œç´¢ã§ç™ºè¦‹"
                    logging.info(f"[GRANT_FINDER] Playwright found: {final_url}")
                    return previous_result
        
        # All retries failed
        notifier.notify_sync(ProgressStage.WARNING, f"[{grant_display_name}] âŒ ä»£æ›¿URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", f"{max_retries}å›ã®æ¤œç´¢ã¨Playwrightæ¢ç´¢ã§ç™ºè¦‹ã§ããš")
        previous_result['is_valid'] = False
        previous_result['url_accessible'] = False
        previous_result['exclude_reason'] = f"URLæ¤œè¨¼å¤±æ•—ï¼ˆ{max_retries}å›ãƒªãƒˆãƒ©ã‚¤ + Playwrightæ¢ç´¢å¤±æ•—ï¼‰"
        
        return previous_result
    
    def _playwright_find_grant_page(self, org_name: str, grant_name: str) -> Optional[str]:
        """
        Use Playwright to find grant page by exploring organization's website.
        """
        try:
            from src.tools.site_explorer import run_sync
            return run_sync(self._async_playwright_find_grant_page(org_name, grant_name))
        except Exception as e:
            logging.error(f"[GRANT_FINDER] Playwright search error: {e}")
            return None
    
    async def _async_playwright_find_grant_page(self, org_name: str, grant_name: str) -> Optional[str]:
        """
        Async Playwright search for grant page.
        Searches Google for organization site, then explores for grant pages.
        """
        try:
            # Search for organization's official site
            search_url = f"https://www.google.com/search?q={org_name}+å…¬å¼ã‚µã‚¤ãƒˆ+åŠ©æˆé‡‘"
            
            grant_info = await self.page_scraper.find_grant_info(search_url, grant_name)
            
            if grant_info.get('accessible'):
                # Look for related links that might be grant pages
                related = grant_info.get('related_links', [])
                for link in related[:5]:
                    href = link.get('href', '')
                    text = link.get('text', '')
                    
                    # Check if link looks like a grant page
                    combined = (href + text).lower()
                    grant_keywords = ['åŠ©æˆ', 'è£œåŠ©', 'æ”¯æ´', 'å‹Ÿé›†', 'å…¬å‹Ÿ', 'ç”³è«‹']
                    
                    if any(kw in combined for kw in grant_keywords):
                        logging.info(f"[GRANT_FINDER] Playwright found potential grant page: {href}")
                        return href
            
            return None
            
        except Exception as e:
            logging.error(f"[GRANT_FINDER] Async Playwright search error: {e}")
            return None

