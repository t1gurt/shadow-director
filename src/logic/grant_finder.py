import re
import logging
import asyncio
from typing import List, Dict, Any, Optional
from google.genai.types import GenerateContentConfig, ThinkingConfig
from src.tools.search_tool import SearchTool
from src.logic.grant_validator import GrantValidator
from src.logic.grant_page_scraper import GrantPageScraper
from src.utils.progress_notifier import get_progress_notifier, ProgressStage

class GrantFinder:
    """
    Handles grant search operations including query generation and official page lookup.
    Uses Playwright-based GrantPageScraper for enhanced page verification.
    Implements SGNA (Search-Ground-Navigate-Act) model for improved accuracy.
    """
    
    # Trusted domains for grant information (SGNA model: Site Restrictions)
    TRUSTED_DOMAINS = [
        'go.jp',      # æ”¿åºœæ©Ÿé–¢
        'or.jp',      # è²¡å›£æ³•äººãƒ»NPO
        'lg.jp',      # åœ°æ–¹è‡ªæ²»ä½“
        'ac.jp',      # å­¦è¡“æ©Ÿé–¢
        'org',        # éå–¶åˆ©çµ„ç¹”
        'co.jp',      # ä¼æ¥­ï¼ˆCSRåŠ©æˆé‡‘ï¼‰
        'com',        # å›½éš›ä¼æ¥­
    ]
    
    def __init__(self, client, model_name: str, config: Dict[str, Any]):
        self.client = client
        self.model_name = model_name
        self.config = config
        self.search_tool = SearchTool()
        self.validator = GrantValidator()
        self.page_scraper = GrantPageScraper()
        self.system_prompt = self.config.get("system_prompts", {}).get("observer", "")

    def generate_queries(self, profile: str) -> List[str]:
        """
        Generates optimized search queries based on the Soul Profile.
        """
        # Get prompt template from config
        prompt_template = self.config.get("system_prompts", {}).get("observer_query_generator", "")
        if prompt_template:
            prompt = prompt_template.format(profile=profile)
        else:
            # Fallback to inline prompt if template not found
            prompt = f"""
ç¾åœ¨ Soul Profile:
{profile}

ã‚¿ã‚¹ã‚¯:
ã“ã®NPOã«æœ€é©ãªè³‡é‡‘èª¿é”æ©Ÿä¼šï¼ˆåŠ©æˆé‡‘ã€CSRï¼‰ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã®3ã¤ã®ç•°ãªã‚‹æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ãƒŸãƒƒã‚·ãƒ§ãƒ³ã€å¯¾è±¡èª²é¡Œã€ç‹¬è‡ªã®å¼·ã¿ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ãã ã•ã„ã€‚
ã‚¯ã‚¨ãƒªã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚1è¡Œã«1ã¤ã®ã‚¯ã‚¨ãƒªã€‚
"""
        try:
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt
            )
            queries = [q.strip() for q in response.text.strip().split('\n') if q.strip()]
            return queries[:3] # Limit to top 3
        except Exception as e:
            logging.error(f"Error generating queries: {e}")
            return [f"NPOåŠ©æˆé‡‘ {profile[:50]}..."] # Fallback

    def parse_opportunities(self, text: str) -> List[Dict]:
        """
        Parse structured opportunity data from Observer response.
        """
        opportunities = []
        
        # Split by ### æ©Ÿä¼š pattern
        sections = re.split(r'###\s*æ©Ÿä¼š\s*\d+:', text)
        
        for section in sections[1:]:  # Skip first empty section
            try:
                # Extract title (first line)
                lines = section.strip().split('\n')
                title = lines[0].strip() if lines else "ä¸æ˜"
                
                # Extract URL
                url_match = re.search(r'\*\*URL\*\*:\s*(.+)', section)
                url = url_match.group(1).strip() if url_match else "N/A"
                
                # Extract amount
                amount_match = re.search(r'\*\*é‡‘é¡\*\*:\s*(.+)', section)
                amount = amount_match.group(1).strip() if amount_match else "N/A"
                
                # Extract resonance score
                score_match = re.search(r'\*\*å…±é³´ã‚¹ã‚³ã‚¢\*\*:\s*(\d+)', section)
                score = int(score_match.group(1)) if score_match else 0
                
                # Extract reason
                reason_match = re.search(r'\*\*å…±é³´ç†ç”±\*\*:\s*(.+)', section)
                reason = reason_match.group(1).strip() if reason_match else "ç†ç”±ä¸æ˜"
                
                opportunities.append({
                    "title": title,
                    "url": url,
                    "amount": amount,
                    "resonance_score": score,
                    "reason": reason
                })
                
                logging.debug(f"[DEBUG] Parsed opportunity: {title} (Score: {score})")
            except Exception as e:
                logging.error(f"[ERROR] Failed to parse opportunity section: {e}")
                continue
        
        return opportunities

    def search_grants(self, profile: str, current_date: str, excluded_grants: str = None) -> tuple[str, List[Dict]]:
        """
        Executes first step of observation: Generates queries and searches for grants.
        Returns the raw response text and parsed opportunities.
        """
        queries = self.generate_queries(profile)
        logging.info(f"Generated Search Queries: {queries}")
        
        # Get prompt template from config
        prompt_template = self.config.get("system_prompts", {}).get("observer_search_task", "")
        
        full_prompt = ""
        if prompt_template:
            if "{current_date}" in prompt_template:
                full_prompt = prompt_template.format(
                    system_prompt=self.system_prompt,
                    profile=profile,
                    queries=', '.join(queries),
                    current_date=current_date,
                    excluded_grants=excluded_grants or "ãªã—"
                )
            else:
                full_prompt = prompt_template.format(
                    system_prompt=self.system_prompt,
                    profile=profile,
                    queries=', '.join(queries)
                )
                full_prompt += f"\n\n**é‡è¦**: æœ¬æ—¥ã¯{current_date}ã§ã™ã€‚ç¾åœ¨å‹Ÿé›†ä¸­ã®åŠ©æˆé‡‘ã®ã¿ã‚’å ±å‘Šã—ã¦ãã ã•ã„ã€‚"
                if excluded_grants:
                    full_prompt += f"\n\n**é™¤å¤–ãƒªã‚¹ãƒˆï¼ˆãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆæ¸ˆã¿ï¼‰**: {excluded_grants}"
        else:
             full_prompt = f"""
{self.system_prompt}

ç¾åœ¨ã®Soul Profile:
{profile}

æ¤œç´¢æˆ¦ç•¥:
ä»¥ä¸‹ã®æ¤œç´¢ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆã—ã¾ã—ãŸ:
{', '.join(queries)}

ã‚¿ã‚¹ã‚¯:
æ¤œç´¢ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€ã“ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã¨å…±é³´ã™ã‚‹ç¾åœ¨ã®NPOåŠ©æˆé‡‘ã‚„CSRè³‡é‡‘èª¿é”æ©Ÿä¼šã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚
ã‚¯ã‚¨ãƒªãŒç¤ºå”†ã™ã‚‹æˆ¦ç•¥ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
è¦‹ã¤ã‹ã£ãŸä¸Šä½3ã¤ã®æ©Ÿä¼šã«ã¤ã„ã¦å ±å‘Šã—ã¦ãã ã•ã„ã€‚
"""

        try:
            # Enable Google Search Tool
            tool_config = self.search_tool.get_tool_config()
            
            # Gemini 3.0 Thinking Mode for grant discovery
            thinking_config = ThinkingConfig(thinking_level="high")
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=full_prompt,
                config=GenerateContentConfig(
                    tools=[tool_config],
                    thinking_config=thinking_config
                )
            )
            response_text = response.text
            
            # Here we could extract grounding metadata as before if needed, 
            # but for now we focus on the text response parsing.
            
            opportunities = self.parse_opportunities(response_text)
            return response_text, opportunities
            
        except Exception as e:
            logging.error(f"Error in search_grants: {e}")
            return f"æ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}", []

    def find_official_page(self, grant_name: str, current_date: str) -> Dict:
        """
        Searches for the official grant page and verifies the application deadline.
        Uses organization name + targeted keywords for better search accuracy.
        """
        logging.info(f"[GRANT_FINDER] Finding official page for: {grant_name}")
        
        result = {
            'official_url': 'N/A',
            'domain': '',
            'deadline_start': '',
            'deadline_end': '',
            'status': 'ä¸æ˜',
            'is_valid': False,
            'confidence': 'ä½',
            'confidence_reason': ''
        }
        
        # Extract organization name for targeted search
        org_name = self.validator.extract_organization_name(grant_name)
        if org_name:
            logging.info(f"[GRANT_FINDER] Extracted org name: {org_name}")
        
        # Build improved search prompt
        prompt_template = self.config.get("system_prompts", {}).get("observer_find_official_page", "")
        
        # Extract current year from current_date for search optimization
        current_year = "2026"
        if current_date:
            year_match = re.search(r'(\d{4})', current_date)
            if year_match:
                current_year = year_match.group(1)
        
        # Build site restriction string for trusted domains (SGNA model)
        site_restriction = " OR ".join([f"site:{d}" for d in self.TRUSTED_DOMAINS])
        
        # Create a more targeted search query with SGNA model enhancements
        if org_name:
            search_hint = f"""
**æ¤œç´¢æˆ¦ç•¥ï¼ˆSGNAãƒ¢ãƒ‡ãƒ« - é‡è¦ï¼‰:**

**Step 1: ä¿¡é ¼ã§ãã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ã‚‰æ¤œç´¢**
æ¤œç´¢ã‚¯ã‚¨ãƒªã«ä»¥ä¸‹ã®ã‚µã‚¤ãƒˆåˆ¶é™ã‚’å«ã‚ã¦ãã ã•ã„ï¼š
`"{org_name} åŠ©æˆé‡‘ å‹Ÿé›† {current_year}" ({site_restriction})`

**Step 2: ç€é™¸ãƒšãƒ¼ã‚¸å„ªå…ˆ**
- PDFã¸ã®ç›´æ¥ãƒªãƒ³ã‚¯ã§ã¯ãªãã€HTMLã®ã€Œå…¬å‹Ÿè¦é ˜ãƒšãƒ¼ã‚¸ã€ã‚’æ¢ã—ã¦ãã ã•ã„
- ç›´ãƒªãƒ³ã‚¯ã¯ãƒªãƒ³ã‚¯åˆ‡ã‚Œãƒªã‚¹ã‚¯ãŒé«˜ãã€æœ€æ–°ç‰ˆã‹ã©ã†ã‹ã®åˆ¤æ–­ãŒå›°é›£ã§ã™

**Step 3: æœ€æ–°æƒ…å ±ã®ç¢ºèª**
- ã€Œ{current_year}å¹´åº¦ã€ã€Œç¬¬â—‹å›ã€ã€Œä»¤å’Œâ—‹å¹´ã€ãªã©ã®è¡¨è¨˜ã‚’ç¢ºèª
- å¤ã„å¹´åº¦ã®ãƒšãƒ¼ã‚¸ã‚’é¿ã‘ã¦ãã ã•ã„

**æ³¨æ„:** åŠ©æˆé‡‘åã€Œ{grant_name}ã€ã§ç›´æ¥æ¤œç´¢ã™ã‚‹ã¨å¤ã„ãƒšãƒ¼ã‚¸ãŒãƒ’ãƒƒãƒˆã—ã‚„ã™ã„ãŸã‚ã€
ã¾ãšçµ„ç¹”ã®åŠ©æˆé‡‘ãƒãƒ¼ã‚¿ãƒ«ãƒšãƒ¼ã‚¸ã‚’è¦‹ã¤ã‘ã€ãã“ã‹ã‚‰è©²å½“ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚
"""
        else:
            search_hint = f"""
**æ¤œç´¢æˆ¦ç•¥ï¼ˆSGNAãƒ¢ãƒ‡ãƒ«ï¼‰:**
åŠ©æˆé‡‘ã€Œ{grant_name}ã€ã®å…¬å¼ãƒšãƒ¼ã‚¸ã‚’ä»¥ä¸‹ã®æ¡ä»¶ã§æ¤œç´¢ã—ã¦ãã ã•ã„ï¼š
- ä¿¡é ¼ã§ãã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³: {site_restriction}
- å¹´åº¦: {current_year}å¹´åº¦ã¾ãŸã¯æœ€æ–°ã®å…¬å‹Ÿ
- HTMLãƒšãƒ¼ã‚¸ã‚’å„ªå…ˆï¼ˆPDFã¸ã®ç›´ãƒªãƒ³ã‚¯ã‚ˆã‚Šç€é™¸ãƒšãƒ¼ã‚¸ã‚’å„ªå…ˆï¼‰
"""
        
        if prompt_template:
            full_prompt = prompt_template.format(
                grant_name=grant_name,
                current_date=current_date
            )
            # Append search strategy hint
            full_prompt = search_hint + "\n" + full_prompt
        else:
            full_prompt = f"""
{search_hint}

åŠ©æˆé‡‘ã€Œ{grant_name}ã€ã®å…¬å¼ç”³è«‹ãƒšãƒ¼ã‚¸ã‚’è¦‹ã¤ã‘ã¦ãã ã•ã„ã€‚

æœ¬æ—¥: {current_date}

**å‡ºåŠ›å½¢å¼:**
- **å…¬å¼URL**: [æ­£ç¢ºãªURL]
- **ãƒ‰ãƒ¡ã‚¤ãƒ³**: [ãƒ‰ãƒ¡ã‚¤ãƒ³å]
- **å‹Ÿé›†é–‹å§‹æ—¥**: [æ—¥ä»˜]
- **å‹Ÿé›†çµ‚äº†æ—¥**: [æ—¥ä»˜]
- **å‹Ÿé›†çŠ¶æ³**: [å‹Ÿé›†ä¸­/å‹Ÿé›†çµ‚äº†/ä»Šå¾Œå‹Ÿé›†äºˆå®š/ä¸æ˜]
- **ä¿¡é ¼åº¦**: [é«˜/ä¸­/ä½]
- **ä¿¡é ¼åº¦ç†ç”±**: [ç†ç”±]
"""
        
        try:
            tool_config = self.search_tool.get_tool_config()
            
            # Gemini 3.0 Thinking Mode for deep reasoning during page investigation
            thinking_config = ThinkingConfig(thinking_level="high")
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=full_prompt,
                config=GenerateContentConfig(
                    tools=[tool_config],
                    temperature=0.2,
                    thinking_config=thinking_config
                )
            )
            
            response_text = response.text
            logging.info(f"[GRANT_FINDER] Response: {response_text[:200]}...")
            
            # Parse response
            url_match = re.search(r'\*\*å…¬å¼URL\*\*:\s*(.+)', response_text)
            if url_match:
                url = url_match.group(1).strip()
                result['official_url'] = self.validator.resolve_redirect_url(url)
            
            domain_match = re.search(r'\*\*ãƒ‰ãƒ¡ã‚¤ãƒ³\*\*:\s*(.+)', response_text)
            if domain_match:
                result['domain'] = domain_match.group(1).strip()
            
            start_match = re.search(r'\*\*å‹Ÿé›†é–‹å§‹æ—¥\*\*:\s*(.+)', response_text)
            if start_match:
                result['deadline_start'] = start_match.group(1).strip()
            
            end_match = re.search(r'\*\*å‹Ÿé›†çµ‚äº†æ—¥\*\*:\s*(.+)', response_text)
            if end_match:
                result['deadline_end'] = end_match.group(1).strip()
            
            status_match = re.search(r'\*\*å‹Ÿé›†çŠ¶æ³\*\*:\s*(.+)', response_text)
            if status_match:
                status = status_match.group(1).strip()
                result['status'] = status
                if 'å‹Ÿé›†ä¸­' in status or 'ä»Šå¾Œ' in status or 'äºˆå®š' in status:
                    result['is_valid'] = True
                elif 'çµ‚äº†' in status or 'ç· åˆ‡' in status:
                    result['is_valid'] = False
            
            confidence_match = re.search(r'\*\*ä¿¡é ¼åº¦\*\*:\s*(.+)', response_text)
            if confidence_match:
                result['confidence'] = confidence_match.group(1).strip()
            
            reason_match = re.search(r'\*\*ä¿¡é ¼åº¦ç†ç”±\*\*:\s*(.+)', response_text)
            if reason_match:
                result['confidence_reason'] = reason_match.group(1).strip()
            
            # Validation Step
            if result['official_url'] != 'N/A':
                notifier = get_progress_notifier()
                
                quality_score, quality_reason = self.validator.evaluate_url_quality(result['official_url'], grant_name)
                result['url_quality_score'] = quality_score
                result['url_quality_reason'] = quality_reason
                
                # Notify user about URL quality
                if quality_score >= 70:
                    notifier.notify_sync(ProgressStage.ANALYZING, f"âœ… ä¿¡é ¼æ€§è©•ä¾¡: {quality_score}ç‚¹", quality_reason)
                elif quality_score >= 50:
                    notifier.notify_sync(ProgressStage.ANALYZING, f"âš ï¸ ä¿¡é ¼æ€§è©•ä¾¡: {quality_score}ç‚¹", quality_reason)
                else:
                    notifier.notify_sync(ProgressStage.WARNING, f"âŒ ä¿¡é ¼æ€§è©•ä¾¡: {quality_score}ç‚¹ï¼ˆä½ï¼‰", quality_reason)
                
                if quality_score < 50:
                    logging.warning(f"[GRANT_FINDER] Low quality URL: {result['official_url']}")
                    result['is_valid'] = False
                
                is_accessible, access_status, final_url = self.validator.validate_url_accessible(result['official_url'])
                result['url_accessible'] = is_accessible
                result['url_access_status'] = access_status
                
                # Notify user about accessibility
                if is_accessible:
                    notifier.notify_sync(ProgressStage.ANALYZING, "âœ… å…¬å¼ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½", f"URL: {final_url[:60]}...")
                else:
                    notifier.notify_sync(ProgressStage.WARNING, "âŒ å…¬å¼ãƒšãƒ¼ã‚¸ã«ã‚¢ã‚¯ã‚»ã‚¹ä¸å¯", access_status)
                
                if is_accessible and final_url:
                    result['official_url'] = final_url
                    
                    # Enhanced verification with Playwright
                    try:
                        logging.info(f"[GRANT_FINDER] Running Playwright verification for: {final_url}")
                        notifier.notify_sync(ProgressStage.ANALYZING, "ğŸ” Playwrightã§è©³ç´°æ¤œè¨¼ä¸­...", "ãƒšãƒ¼ã‚¸å†…å®¹ã‚’è§£æã—ã¦ã„ã¾ã™")
                        
                        playwright_result = self._run_playwright_verification(final_url, grant_name)
                        
                        if playwright_result:
                            result['playwright_verified'] = True
                            result['playwright_confidence'] = playwright_result.get('confidence', 0)
                            result['format_files'] = playwright_result.get('format_files', [])
                            
                            # Notify Playwright results
                            file_count = len(result.get('format_files', []))
                            if file_count > 0:
                                notifier.notify_sync(ProgressStage.ANALYZING, f"ğŸ“ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ« {file_count}ä»¶ ç™ºè¦‹", "ç”³è«‹æ›¸æ§˜å¼ã‚’æ¤œå‡ºã—ã¾ã—ãŸ")
                            
                            # Update deadline info if found
                            if playwright_result.get('deadline_info'):
                                deadline = playwright_result['deadline_info']
                                if deadline.get('date'):
                                    result['deadline_end'] = deadline['date']
                                    notifier.notify_sync(ProgressStage.ANALYZING, f"ğŸ“… ç· åˆ‡æ—¥: {deadline['date']}", "ãƒšãƒ¼ã‚¸ã‹ã‚‰ç· åˆ‡æ—¥ã‚’æŠ½å‡ºã—ã¾ã—ãŸ")
                            
                            logging.info(f"[GRANT_FINDER] Playwright found {file_count} format files")
                        else:
                            notifier.notify_sync(ProgressStage.ANALYZING, "â„¹ï¸ Playwrightæ¤œè¨¼å®Œäº†", "è¿½åŠ æƒ…å ±ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
                    except Exception as pw_error:
                        logging.warning(f"[GRANT_FINDER] Playwright verification failed: {pw_error}")
                        result['playwright_verified'] = False
                else:
                    # Retry logic
                    return self._retry_find_official_page(grant_name, result, access_status)

            logging.info(f"[GRANT_FINDER] Result: URL={result['official_url'][:50]}..., Valid={result['is_valid']}")
            
        except Exception as e:
            logging.error(f"[GRANT_FINDER] Error finding official page: {e}")
        
        return result
    
    def _run_playwright_verification(self, url: str, grant_name: str) -> Optional[Dict[str, Any]]:
        """
        Run Playwright-based page verification.
        Uses nest_asyncio to allow running async code within Discord.py's event loop.
        """
        try:
            import nest_asyncio
            nest_asyncio.apply()
            
            # Now we can safely run asyncio.run() within the existing event loop
            return asyncio.run(self._async_playwright_verification(url, grant_name))
        except Exception as e:
            logging.error(f"[GRANT_FINDER] Playwright verification error: {e}")
            return None
    
    async def _async_playwright_verification(self, url: str, grant_name: str) -> Optional[Dict[str, Any]]:
        """
        Async Playwright verification.
        """
        try:
            grant_info = await self.page_scraper.find_grant_info(url, grant_name)
            
            if not grant_info.get('accessible'):
                return None
            
            return {
                'verified': True,
                'confidence': 80 if grant_info.get('format_files') else 50,
                'format_files': grant_info.get('format_files', []),
                'deadline_info': grant_info.get('deadline_info'),
                'related_links': grant_info.get('related_links', [])
            }
        except Exception as e:
            logging.error(f"[GRANT_FINDER] Async Playwright error: {e}")
            return None

    def _retry_find_official_page(self, grant_name: str, previous_result: Dict, failure_reason: str) -> Dict:
        """
        Retries finding the official page if the first attempt failed validation.
        Enhanced with:
        1. Multiple search query variations
        2. Playwright-based site exploration
        3. Up to 3 retry attempts
        """
        logging.info(f"[GRANT_FINDER] Retrying for: {grant_name}")
        notifier = get_progress_notifier()
        notifier.notify_sync(ProgressStage.WARNING, f"URLæ¤œè¨¼å¤±æ•—: {grant_name[:20]}...", f"ä»£æ›¿URLã‚’æ¤œç´¢ä¸­... ({failure_reason})")
        
        # Extract organization name for targeted retry search
        org_name = self.validator.extract_organization_name(grant_name)
        
        # Define multiple search strategies
        search_queries = []
        if org_name:
            search_queries = [
                f"{org_name} åŠ©æˆé‡‘ å‹Ÿé›† 2026",
                f"{org_name} è£œåŠ©é‡‘ ç”³è«‹",
                f"{org_name} æ”¯æ´ å…¬å‹Ÿ",
                f"{org_name} å…¬å¼ã‚µã‚¤ãƒˆ åŠ©æˆ",
            ]
        else:
            search_queries = [
                f"{grant_name} å…¬å¼",
                f"{grant_name} ç”³è«‹",
            ]
        
        # Try up to 3 different search strategies
        max_retries = min(3, len(search_queries))
        
        for retry_num in range(max_retries):
            query = search_queries[retry_num]
            notifier.notify_sync(ProgressStage.SEARCHING, f"ä»£æ›¿æ¤œç´¢ ({retry_num + 1}/{max_retries})", f"æ¤œç´¢: {query[:40]}...")
            logging.info(f"[GRANT_FINDER] Retry {retry_num + 1}: searching with '{query}'")
            
            # Build site restriction for retry (SGNA model)
            site_restriction = " OR ".join([f"site:{d}" for d in self.TRUSTED_DOMAINS])
            
            retry_prompt = f"""
åŠ©æˆé‡‘ã®å…¬å¼ç”³è«‹ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã—ã¦ãã ã•ã„ã€‚

**æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆSGNAãƒ¢ãƒ‡ãƒ«ï¼‰:** `"{query}" ({site_restriction})`

**æ¢ã—ã¦ã„ã‚‹åŠ©æˆé‡‘:** {grant_name}

**é‡è¦æ¡ä»¶:**
1. ä¿¡é ¼ã§ãã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ã¿: go.jp, or.jp, lg.jp, co.jp, org, com
2. **ç€é™¸ãƒšãƒ¼ã‚¸å„ªå…ˆ**: PDFã¸ã®ç›´ãƒªãƒ³ã‚¯ã§ã¯ãªãã€HTMLã®å…¬å‹Ÿãƒšãƒ¼ã‚¸ã‚’é¸æŠ
3. æœ€æ–°ã®å…¬å‹Ÿæƒ…å ±ã§ã‚ã‚‹ã“ã¨ï¼ˆå¹´åº¦ã‚’ç¢ºèªï¼‰

**å‡ºåŠ›å½¢å¼:**
- **å…¬å¼URL**: [æ­£ç¢ºãªURL]
"""
            try:
                # Gemini 3.0 Thinking Mode for retry search
                thinking_config = ThinkingConfig(thinking_level="high")
                
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=retry_prompt,
                    config=GenerateContentConfig(
                        tools=[self.search_tool.get_tool_config()],
                        temperature=0.1,
                        thinking_config=thinking_config
                    )
                )
                
                retry_url_match = re.search(r'\*\*å…¬å¼URL\*\*:\s*(.+)', response.text)
                if retry_url_match:
                    retry_url = retry_url_match.group(1).strip()
                    retry_url = self.validator.resolve_redirect_url(retry_url)
                    
                    # Skip if same as failed URL
                    if retry_url == previous_result.get('official_url'):
                        logging.info(f"[GRANT_FINDER] Same URL found, trying next query")
                        continue
                    
                    is_retry_accessible, retry_status, retry_final_url = self.validator.validate_url_accessible(retry_url)
                    
                    if is_retry_accessible and retry_final_url:
                        notifier.notify_sync(ProgressStage.ANALYZING, f"âœ… ä»£æ›¿URLç™ºè¦‹ (è©¦è¡Œ{retry_num + 1})", retry_final_url[:60])
                        
                        previous_result['official_url'] = retry_final_url
                        previous_result['url_accessible'] = True
                        previous_result['url_access_status'] = f"ãƒªãƒˆãƒ©ã‚¤æˆåŠŸï¼ˆè©¦è¡Œ{retry_num + 1}ï¼‰"
                        logging.info(f"[GRANT_FINDER] Retry {retry_num + 1} successful: {retry_final_url}")
                        return previous_result
                    else:
                        logging.info(f"[GRANT_FINDER] Retry {retry_num + 1} URL not accessible: {retry_status}")
                        
            except Exception as retry_e:
                logging.error(f"[GRANT_FINDER] Retry {retry_num + 1} error: {retry_e}")
        
        # All LLM retries failed - try Playwright exploration as last resort
        if org_name:
            notifier.notify_sync(ProgressStage.SEARCHING, "ğŸ” Playwrightæ·±æ˜ã‚Šæ¤œç´¢ä¸­...", f"çµ„ç¹”ã‚µã‚¤ãƒˆã‚’æ¢ç´¢: {org_name}")
            playwright_url = self._playwright_find_grant_page(org_name, grant_name)
            
            if playwright_url:
                is_accessible, status, final_url = self.validator.validate_url_accessible(playwright_url)
                if is_accessible and final_url:
                    notifier.notify_sync(ProgressStage.ANALYZING, "âœ… Playwrightã§ä»£æ›¿URLç™ºè¦‹", final_url[:60])
                    previous_result['official_url'] = final_url
                    previous_result['url_accessible'] = True
                    previous_result['url_access_status'] = "Playwrightæ¤œç´¢ã§ç™ºè¦‹"
                    logging.info(f"[GRANT_FINDER] Playwright found: {final_url}")
                    return previous_result
        
        # All retries failed
        notifier.notify_sync(ProgressStage.WARNING, "âŒ ä»£æ›¿URLãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ", f"{max_retries}å›ã®æ¤œç´¢ã¨Playwrightæ¢ç´¢ã§ç™ºè¦‹ã§ããš")
        previous_result['is_valid'] = False
        previous_result['url_accessible'] = False
        previous_result['exclude_reason'] = f"URLæ¤œè¨¼å¤±æ•—ï¼ˆ{max_retries}å›ãƒªãƒˆãƒ©ã‚¤ + Playwrightæ¢ç´¢å¤±æ•—ï¼‰"
        
        return previous_result
    
    def _playwright_find_grant_page(self, org_name: str, grant_name: str) -> Optional[str]:
        """
        Use Playwright to find grant page by exploring organization's website.
        """
        try:
            import nest_asyncio
            nest_asyncio.apply()
            return asyncio.run(self._async_playwright_find_grant_page(org_name, grant_name))
        except Exception as e:
            logging.error(f"[GRANT_FINDER] Playwright search error: {e}")
            return None
    
    async def _async_playwright_find_grant_page(self, org_name: str, grant_name: str) -> Optional[str]:
        """
        Async Playwright search for grant page.
        Searches Google for organization site, then explores for grant pages.
        """
        try:
            # Search for organization's official site
            search_url = f"https://www.google.com/search?q={org_name}+å…¬å¼ã‚µã‚¤ãƒˆ+åŠ©æˆé‡‘"
            
            grant_info = await self.page_scraper.find_grant_info(search_url, grant_name)
            
            if grant_info.get('accessible'):
                # Look for related links that might be grant pages
                related = grant_info.get('related_links', [])
                for link in related[:5]:
                    href = link.get('href', '')
                    text = link.get('text', '')
                    
                    # Check if link looks like a grant page
                    combined = (href + text).lower()
                    grant_keywords = ['åŠ©æˆ', 'è£œåŠ©', 'æ”¯æ´', 'å‹Ÿé›†', 'å…¬å‹Ÿ', 'ç”³è«‹']
                    
                    if any(kw in combined for kw in grant_keywords):
                        logging.info(f"[GRANT_FINDER] Playwright found potential grant page: {href}")
                        return href
            
            return None
            
        except Exception as e:
            logging.error(f"[GRANT_FINDER] Async Playwright search error: {e}")
            return None

