"""
æ•µå¯¾çš„è©•ä¾¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ (Critic Agent)
ã€Œå†·å¾¹ãªè²¡å›£å¯©æŸ»å“¡ã€ã¨ã—ã¦ç”³è«‹æ›¸ã‚’æ‰¹åˆ¤çš„ã«è©•ä¾¡ã—ã€ä¿®æ­£æŒ‡ç¤ºã‚’ç”Ÿæˆ
"""

import os
import re
import logging
from typing import Dict, Any, Optional, Callable, List
from dataclasses import dataclass, field
import yaml

from google import genai
from google.genai.types import GenerateContentConfig


@dataclass
class CritiqueResult:
    """è©•ä¾¡çµæœ"""
    score: int  # 0-100
    verdict: str  # 'pass' or 'reject'
    critique: str  # æ‰¹åˆ¤ã‚³ãƒ¡ãƒ³ãƒˆ
    improvement_points: List[str]  # ä¿®æ­£æŒ‡ç¤º
    reasoning: str  # æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹
    scores_detail: Dict[str, int] = field(default_factory=dict)  # 5è»¸ã‚¹ã‚³ã‚¢è©³ç´°


@dataclass
class DialogueEntry:
    """è­°è«–ãƒ­ã‚°ã®ã‚¨ãƒ³ãƒˆãƒª"""
    round: int
    role: str  # 'writer' or 'critic'
    content: str
    score: int = 0


@dataclass
class RevisionResult:
    """æ¨æ•²ãƒ«ãƒ¼ãƒ—ã®çµæœ"""
    final_draft: str
    final_score: int
    iterations: int
    dialogue_log: List[DialogueEntry]
    passed: bool
    best_draft: str = ""  # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ãƒ‰ãƒ©ãƒ•ãƒˆ
    best_score: int = 0


class CriticAgent:
    """
    æ•µå¯¾çš„è©•ä¾¡ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
    
    ã€Œå†·å¾¹ãªè²¡å›£å¯©æŸ»å“¡ã€ã®ãƒšãƒ«ã‚½ãƒŠã§ã€ç”³è«‹æ›¸ãƒ‰ãƒ©ãƒ•ãƒˆã‚’
    æ‰¹åˆ¤çš„ã«è©•ä¾¡ã—ã€å…·ä½“çš„ãªä¿®æ­£æŒ‡ç¤ºã‚’ç”Ÿæˆã™ã‚‹ã€‚
    """
    
    def __init__(self):
        self.config = self._load_config()
        self._init_client()
        
    def _load_config(self) -> Dict[str, Any]:
        """è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        try:
            with open("config/prompts.yaml", "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            logging.error(f"Failed to load config: {e}")
            return {}
    
    def _init_client(self):
        """GenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–"""
        model_config = self.config.get("model_config", {})
        project_id = model_config.get("project_id", "zenn-shadow-director")
        location = model_config.get("location", "us-central1")
        
        os.environ["GOOGLE_CLOUD_PROJECT"] = project_id
        os.environ["GOOGLE_CLOUD_LOCATION"] = location
        os.environ["GOOGLE_GENAI_USE_VERTEXAI"] = "True"
        
        try:
            self.client = genai.Client()
        except Exception as e:
            logging.error(f"Failed to initialize client: {e}")
            self.client = None
            
        self.model_name = model_config.get("observer_model", "gemini-3-pro-preview")
    
    def critique_draft(
        self,
        draft_content: str,
        evaluation_criteria: str,
        grant_name: str,
        profile: str,
        competitive_insight: str = "",
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> CritiqueResult:
        """
        ç”³è«‹æ›¸ãƒ‰ãƒ©ãƒ•ãƒˆã‚’å¯©æŸ»å“¡è¦–ç‚¹ã§è©•ä¾¡
        
        Args:
            draft_content: è©•ä¾¡å¯¾è±¡ã®ãƒ‰ãƒ©ãƒ•ãƒˆ
            evaluation_criteria: è©•ä¾¡åŸºæº–ï¼ˆå…¬å‹Ÿè¦é ˜ï¼‰
            grant_name: åŠ©æˆé‡‘å
            profile: NPOãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            competitive_insight: ç«¶åˆèª¿æŸ»çµæœï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            progress_callback: Discordé€šçŸ¥ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
        Returns:
            CritiqueResult: è©•ä¾¡çµæœã¨ä¿®æ­£æŒ‡ç¤º
        """
        if not self.client:
            return CritiqueResult(
                score=0,
                verdict="reject",
                critique="è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“",
                improvement_points=[],
                reasoning=""
            )
        
        try:
            # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—ï¼ˆã‚«ã‚¹ã‚¿ãƒ ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            critic_prompt = self.config.get("system_prompts", {}).get("critic", self._get_default_prompt())
            
            # è©•ä¾¡ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
            prompt = f"""{critic_prompt}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# è©•ä¾¡å¯¾è±¡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

**åŠ©æˆé‡‘å:** {grant_name}

**å…¬å‹Ÿè¦é ˜ãƒ»è©•ä¾¡åŸºæº–:**
{evaluation_criteria[:2000] if evaluation_criteria else "è©•ä¾¡åŸºæº–æƒ…å ±ãªã—"}

**ç«¶åˆèª¿æŸ»çµæœ:**
{competitive_insight[:1000] if competitive_insight else "ç«¶åˆæƒ…å ±ãªã—"}

**ç”³è«‹å›£ä½“ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«:**
{profile[:1500]}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# è©•ä¾¡å¯¾è±¡ãƒ‰ãƒ©ãƒ•ãƒˆ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{draft_content[:4000]}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
# å‡ºåŠ›å½¢å¼ï¼ˆJSONï¼‰
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
```json
{{
  "scores_detail": {{
    "social_impact": XX,
    "budget_validity": XX,
    "feasibility": XX,
    "uniqueness": XX,
    "credibility": XX
  }},
  "total_score": XX,
  "verdict": "pass ã¾ãŸã¯ reject",
  "reasoning": "å¯©æŸ»å“¡ã¨ã—ã¦ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ï¼ˆå„è¦³ç‚¹ã®è©•ä¾¡ç†ç”±ã‚’éšå±¤çš„ã«è¨˜è¿°ï¼‰",
  "critique": "ç·è©•ã‚³ãƒ¡ãƒ³ãƒˆ",
  "improvement_points": [
    "å…·ä½“çš„ãªä¿®æ­£æŒ‡ç¤º1",
    "å…·ä½“çš„ãªä¿®æ­£æŒ‡ç¤º2",
    "å…·ä½“çš„ãªä¿®æ­£æŒ‡ç¤º3"
  ]
}}
```
"""
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=GenerateContentConfig(
                    temperature=0.3,
                    thinking_config={"thinking_budget": 2048}
                )
            )
            
            # JSONã‚’ãƒ‘ãƒ¼ã‚¹
            response_text = response.text
            json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            
            if json_match:
                import json
                data = json.loads(json_match.group(1))
                
                score = data.get("total_score", 0)
                verdict = "pass" if score >= 80 else "reject"
                
                result = CritiqueResult(
                    score=score,
                    verdict=verdict,
                    critique=data.get("critique", ""),
                    improvement_points=data.get("improvement_points", []),
                    reasoning=data.get("reasoning", ""),
                    scores_detail=data.get("scores_detail", {})
                )
                
                # æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¦é€šçŸ¥
                if progress_callback:
                    formatted_thinking = self._format_thinking_process(result)
                    progress_callback(formatted_thinking)
                
                return result
            
            return CritiqueResult(
                score=0,
                verdict="reject",
                critique="è©•ä¾¡çµæœã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸ",
                improvement_points=[],
                reasoning=""
            )
            
        except Exception as e:
            logging.error(f"Critique failed: {e}")
            return CritiqueResult(
                score=0,
                verdict="reject",
                critique=f"è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {str(e)}",
                improvement_points=[],
                reasoning=""
            )
    
    def _format_thinking_process(self, result: CritiqueResult) -> str:
        """æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’Discordé€šçŸ¥ç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        scores = result.scores_detail
        
        # ã‚¹ã‚³ã‚¢è¡¨ç¤ºç”¨ã®ã‚¢ã‚¤ã‚³ãƒ³
        def score_icon(score: int) -> str:
            if score >= 18:
                return "ğŸŸ¢"
            elif score >= 14:
                return "ğŸŸ¡"
            else:
                return "ğŸ”´"
        
        # æ—¥æœ¬èªãƒ©ãƒ™ãƒ«
        labels = {
            "social_impact": "ç¤¾ä¼šçš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ",
            "budget_validity": "äºˆç®—ã®å¦¥å½“æ€§",
            "feasibility": "å®Ÿç¾å¯èƒ½æ€§",
            "uniqueness": "ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ–",
            "credibility": "å›£ä½“ã®ä¿¡é ¼æ€§"
        }
        
        scores_text = "\n".join([
            f"â”‚  {score_icon(score)} {labels.get(key, key)}: {score}/20ç‚¹"
            for key, score in scores.items()
        ])
        
        verdict_icon = "âœ…" if result.verdict == "pass" else "âŒ"
        verdict_text = "åˆæ ¼" if result.verdict == "pass" else "ä¸æ¡æŠ"
        
        improvements = "\n".join([
            f"   {i+1}. {point}" 
            for i, point in enumerate(result.improvement_points[:5])
        ]) if result.improvement_points else "   ãªã—"
        
        return f"""
ğŸ” **Critic æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹:**
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
{scores_text}
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â”‚  ğŸ“Š **ç·åˆã‚¹ã‚³ã‚¢: {result.score}ç‚¹**
â”‚  {verdict_icon} **åˆ¤å®š: {verdict_text}**
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ğŸ“ **ä¿®æ­£æŒ‡ç¤º:**
{improvements}
"""
    
    def _get_default_prompt(self) -> str:
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®Criticãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
        return """ã‚ãªãŸã¯ã€Œå†·å¾¹ãªè²¡å›£å¯©æŸ»å“¡ã€ã¨ã—ã¦ã€NPOåŠ©æˆé‡‘ç”³è«‹æ›¸ã‚’å³ã—ãè©•ä¾¡ã—ã¾ã™ã€‚

**ãƒšãƒ«ã‚½ãƒŠ:**
- 20å¹´ä»¥ä¸Šã®åŠ©æˆé‡‘å¯©æŸ»çµŒé¨“ã‚’æŒã¤è²¡å›£äº‹å‹™å±€é•·
- æ¡æŠç‡10%ä»¥ä¸‹ã®å³ã—ã„å¯©æŸ»ã‚’æ‹…å½“
- ã€ŒãŠæƒ…ã‘æ¡æŠã€ã¯ä¸€åˆ‡ã—ãªã„
- å…·ä½“æ€§ã¨æ ¹æ‹ ã®ãªã„ç”³è«‹ã¯å®¹èµ¦ãªãå´ä¸‹

**è©•ä¾¡è¦³ç‚¹ï¼ˆå„20ç‚¹ã€åˆè¨ˆ100ç‚¹ï¼‰:**
1. **ç¤¾ä¼šçš„ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ (social_impact)**: æ•°å€¤ç›®æ¨™ã€å—ç›Šè€…æ•°ã€æˆæœæŒ‡æ¨™ã®å…·ä½“æ€§
2. **äºˆç®—ã®å¦¥å½“æ€§ (budget_validity)**: ç©ç®—æ ¹æ‹ ã€è²»ç”¨å¯¾åŠ¹æœã®æ˜ç¢ºã•
3. **å®Ÿç¾å¯èƒ½æ€§ (feasibility)**: å®Ÿæ–½ä½“åˆ¶ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å…·ä½“æ€§
4. **ç‹¬è‡ªæ€§ãƒ»å·®åˆ¥åŒ– (uniqueness)**: ä»–å›£ä½“ã¨ã®é•ã„ã€æ–°è¦æ€§
5. **å›£ä½“ã®ä¿¡é ¼æ€§ (credibility)**: å®Ÿç¸¾ã€å°‚é–€æ€§ã€ç¶™ç¶šæ€§

**80ç‚¹ä»¥ä¸Šã§åˆæ ¼ã€ãã‚Œæœªæº€ã¯ä¸æ¡æŠã¨ã—ã¦ä¿®æ­£æŒ‡ç¤ºã‚’å‡ºã™ã€‚**
"""
    
    def revise_draft(
        self,
        original_draft: str,
        critique: CritiqueResult,
        grant_name: str,
        profile: str,
        evaluation_criteria: str,
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> str:
        """
        Criticã®æŒ‡æ‘˜ã‚’å—ã‘ã¦ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ä¿®æ­£
        
        Args:
            original_draft: å…ƒã®ãƒ‰ãƒ©ãƒ•ãƒˆ
            critique: Criticã‹ã‚‰ã®è©•ä¾¡çµæœ
            grant_name: åŠ©æˆé‡‘å
            profile: NPOãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            evaluation_criteria: è©•ä¾¡åŸºæº–
            progress_callback: Discordé€šçŸ¥ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
        Returns:
            ä¿®æ­£å¾Œã®ãƒ‰ãƒ©ãƒ•ãƒˆ
        """
        if not self.client:
            return original_draft
        
        if progress_callback:
            progress_callback("âœï¸ **Writer: æŒ‡æ‘˜äº‹é …ã‚’ä¿®æ­£ä¸­...**")
        
        try:
            # ä¿®æ­£æŒ‡ç¤ºã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
            improvements = "\n".join([
                f"{i+1}. {point}" 
                for i, point in enumerate(critique.improvement_points)
            ])
            
            prompt = f"""ã‚ãªãŸã¯åŠ©æˆé‡‘ç”³è«‹æ›¸ã®ãƒªãƒ©ã‚¤ã‚¿ãƒ¼ã§ã™ã€‚
å¯©æŸ»å“¡ã‹ã‚‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’å—ã‘ã¦ã€ç”³è«‹æ›¸ã‚’æ”¹å–„ã—ã¦ãã ã•ã„ã€‚

**åŠ©æˆé‡‘å:** {grant_name}

**ç¾åœ¨ã®ã‚¹ã‚³ã‚¢:** {critique.score}ç‚¹ï¼ˆç›®æ¨™: 80ç‚¹ä»¥ä¸Šï¼‰

**å¯©æŸ»å“¡ã‹ã‚‰ã®ç·è©•:**
{critique.critique}

**ä¿®æ­£ã™ã¹ãç‚¹:**
{improvements}

**ç”³è«‹å›£ä½“ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«:**
{profile[:2000]}

**å…ƒã®ãƒ‰ãƒ©ãƒ•ãƒˆ:**
{original_draft[:4000]}

**æ”¹å–„ã®åŸå‰‡:**
- æŒ‡æ‘˜ã•ã‚ŒãŸç‚¹ã‚’çš„ç¢ºã«ä¿®æ­£
- å…·ä½“çš„ãªæ•°å€¤ãƒ»æ ¹æ‹ ã‚’è¿½åŠ 
- ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’æœ€å¤§é™æ´»ç”¨
- æ–‡å­—æ•°åˆ¶é™ã‚’éµå®ˆ
- å…ƒã®è‰¯ã„éƒ¨åˆ†ã¯ç¶­æŒ

**å‡ºåŠ›:**
ä¿®æ­£å¾Œã®ãƒ‰ãƒ©ãƒ•ãƒˆå…¨æ–‡ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=GenerateContentConfig(temperature=0.4)
            )
            
            revised_draft = response.text.strip()
            
            if progress_callback:
                progress_callback("âœ… **Writer: ä¿®æ­£ç‰ˆã‚’æå‡ºã—ã¾ã—ãŸ**")
            
            return revised_draft
            
        except Exception as e:
            logging.error(f"Draft revision failed: {e}")
            return original_draft
    
    def run_revision_loop(
        self,
        initial_draft: str,
        evaluation_criteria: str,
        grant_name: str,
        profile: str,
        competitive_insight: str = "",
        max_iterations: int = 3,
        pass_threshold: int = 80,
        progress_callback: Optional[Callable[[str], None]] = None
    ) -> RevisionResult:
        """
        Writer-Criticã®æ¨æ•²ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œ
        
        Args:
            initial_draft: åˆå›ãƒ‰ãƒ©ãƒ•ãƒˆ
            evaluation_criteria: è©•ä¾¡åŸºæº–
            grant_name: åŠ©æˆé‡‘å
            profile: NPOãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            competitive_insight: ç«¶åˆèª¿æŸ»çµæœ
            max_iterations: æœ€å¤§ãƒ«ãƒ¼ãƒ—å›æ•°
            pass_threshold: åˆæ ¼é–¾å€¤ã‚¹ã‚³ã‚¢
            progress_callback: Discordé€šçŸ¥ç”¨ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            
        Returns:
            RevisionResult: æ¨æ•²çµæœ
        """
        dialogue_log: List[DialogueEntry] = []
        current_draft = initial_draft
        best_draft = initial_draft
        best_score = 0
        
        def notify(msg: str):
            if progress_callback:
                progress_callback(msg)
        
        for iteration in range(max_iterations):
            round_num = iteration + 1
            
            # ãƒ©ã‚¦ãƒ³ãƒ‰é–‹å§‹é€šçŸ¥
            notify(f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ”„ **æ¨æ•²ãƒ«ãƒ¼ãƒ— Round {round_num}/{max_iterations}**
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”""")
            
            # Writeræå‡ºã‚’è¨˜éŒ²
            dialogue_log.append(DialogueEntry(
                round=round_num,
                role="writer",
                content=f"{'åˆå›' if round_num == 1 else 'ä¿®æ­£'}ãƒ‰ãƒ©ãƒ•ãƒˆã‚’æå‡º",
                score=0
            ))
            
            if round_num == 1:
                notify("âœï¸ **Writer: åˆå›ãƒ‰ãƒ©ãƒ•ãƒˆã‚’æå‡ºã—ã¾ã—ãŸ**")
            
            # Criticã«ã‚ˆã‚‹è©•ä¾¡
            critique = self.critique_draft(
                draft_content=current_draft,
                evaluation_criteria=evaluation_criteria,
                grant_name=grant_name,
                profile=profile,
                competitive_insight=competitive_insight,
                progress_callback=notify
            )
            
            # è©•ä¾¡çµæœã‚’è¨˜éŒ²
            dialogue_log.append(DialogueEntry(
                round=round_num,
                role="critic",
                content=critique.critique,
                score=critique.score
            ))
            
            # ãƒ™ã‚¹ãƒˆã‚¹ã‚³ã‚¢ã‚’æ›´æ–°
            if critique.score > best_score:
                best_score = critique.score
                best_draft = current_draft
            
            # åˆæ ¼åˆ¤å®š
            if critique.score >= pass_threshold:
                notify(f"""
âœ… **åˆæ ¼ï¼ æœ€çµ‚ã‚¹ã‚³ã‚¢: {critique.score}ç‚¹**
æ¨æ•²ãƒ©ã‚¦ãƒ³ãƒ‰: {round_num}å›
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”""")
                
                return RevisionResult(
                    final_draft=current_draft,
                    final_score=critique.score,
                    iterations=round_num,
                    dialogue_log=dialogue_log,
                    passed=True,
                    best_draft=current_draft,
                    best_score=critique.score
                )
            
            # æœ€å¾Œã®ãƒ©ã‚¦ãƒ³ãƒ‰ãªã‚‰ä¿®æ­£ã›ãšçµ‚äº†
            if round_num >= max_iterations:
                break
            
            # Writerã«ã‚ˆã‚‹ä¿®æ­£
            current_draft = self.revise_draft(
                original_draft=current_draft,
                critique=critique,
                grant_name=grant_name,
                profile=profile,
                evaluation_criteria=evaluation_criteria,
                progress_callback=notify
            )
        
        # æœ€å¤§ãƒ©ã‚¦ãƒ³ãƒ‰åˆ°é”ï¼ˆä¸åˆæ ¼ï¼‰
        notify(f"""
âš ï¸ **æ¨æ•²ãƒ«ãƒ¼ãƒ—å®Œäº†ï¼ˆç›®æ¨™ã‚¹ã‚³ã‚¢æœªé”ï¼‰**
æœ€çµ‚ã‚¹ã‚³ã‚¢: {best_score}ç‚¹ï¼ˆç›®æ¨™: {pass_threshold}ç‚¹ï¼‰
æœ€é«˜ã‚¹ã‚³ã‚¢ã®ãƒ‰ãƒ©ãƒ•ãƒˆã‚’æ¡ç”¨ã—ã¾ã™ã€‚
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”""")
        
        return RevisionResult(
            final_draft=best_draft,
            final_score=best_score,
            iterations=max_iterations,
            dialogue_log=dialogue_log,
            passed=False,
            best_draft=best_draft,
            best_score=best_score
        )
    
    def format_dialogue_log(self, dialogue_log: List[DialogueEntry]) -> str:
        """è­°è«–ãƒ­ã‚°ã‚’Discordè¡¨ç¤ºç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
        lines = ["ğŸ“œ **AIè­°è«–ãƒ­ã‚°**", ""]
        
        for entry in dialogue_log:
            if entry.role == "writer":
                lines.append(f"**Round {entry.round}** âœï¸ Writer: {entry.content}")
            else:
                icon = "âœ…" if entry.score >= 80 else "âŒ"
                lines.append(f"  ğŸ” Critic: {entry.score}ç‚¹ {icon}")
                if entry.content:
                    lines.append(f"     â†’ {entry.content[:100]}...")
        
        return "\n".join(lines)
