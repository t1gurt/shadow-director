from typing import Dict, Any, Optional, Tuple, List
import yaml
import os
import asyncio
import logging
from src.tools.gdocs_tool import GoogleDocsTool
from src.memory.profile_manager import ProfileManager
from src.tools.file_downloader import FileDownloader
from src.logic.grant_page_scraper import GrantPageScraper
from src.logic.format_field_mapper import FormatFieldMapper
from src.tools.document_filler import DocumentFiller
from src.logic.file_classifier import FileClassifier

class DrafterAgent:
    def __init__(self):
        self.config = self._load_config()
        self.system_prompt = self.config.get("system_prompts", {}).get("drafter", "")
        
        # Initialize Gemini client via Vertex AI backend
        try:
            from src.utils.gemini_client import get_gemini_client
            self.client = get_gemini_client()
            logging.info("[DRAFTER] Gemini client initialized via Vertex AI")
        except Exception as e:
            logging.error(f"[DRAFTER] Failed to init Gemini client: {e}")
            self.client = None
            
        # Using Interviewer model (Pro) for drafting as it requires high reasoning/writing capability
        self.model_name = self.config.get("model_config", {}).get("interviewer_model")
        if not self.model_name:
             raise ValueError("interviewer_model (for drafter) not found in config")
        self.docs_tool = GoogleDocsTool()
        self.file_downloader = FileDownloader()
        
        # Initialize page scraper with Gemini client for visual fallback
        # Use shorter timeout (10s) for drafter operations to avoid long hangs
        self.page_scraper = GrantPageScraper(
            gemini_client=self.client, 
            model_name=self.model_name,
            timeout=10000  # 10 seconds timeout for Playwright operations
        )
        
        # Initialize format field mapper and document filler
        self.format_mapper = FormatFieldMapper(
            gemini_client=self.client,
            model_name=self.model_name
        )
        self.document_filler = DocumentFiller()
        
        # Initialize file classifier for early filtering
        vlm_model = self.config.get("model_config", {}).get("vlm_model", "gemini-3-flash-preview")
        self.file_classifier = FileClassifier(self.client, vlm_model)

    def _load_config(self) -> Dict[str, Any]:
        try:
            with open("config/prompts.yaml", "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"Error loading config: {e}")
            return {}

    def _sanitize_grant_name_for_search(self, grant_name: str) -> str:
        """
        æ¤œç´¢ã‚¯ã‚¨ãƒªã§ä½¿ç”¨ã™ã‚‹ãŸã‚ã«grant_nameã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã™ã‚‹ã€‚
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚³ãƒãƒ³ãƒ‰ï¼ˆã€Œãƒ‰ãƒ©ãƒ•ãƒˆã‚’ä½œæˆã—ã¦ã€ç­‰ï¼‰ã‚’é™¤å»ã€‚
        
        Args:
            grant_name: ã‚µãƒ‹ã‚¿ã‚¤ã‚ºå¯¾è±¡ã®åŠ©æˆé‡‘å
            
        Returns:
            ã‚µãƒ‹ã‚¿ã‚¤ã‚ºæ¸ˆã¿ã®åŠ©æˆé‡‘å
        """
        import re
        
        if not grant_name:
            return ""
        
        # é™¤å»ã™ã¹ããƒ•ãƒ¬ãƒ¼ã‚ºï¼ˆã‚³ãƒãƒ³ãƒ‰ç³»ï¼‰
        remove_phrases = [
            r'ã®ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ä½œæˆã—ã¦',
            r'ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ä½œæˆã—ã¦',
            r'ã®ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆ',
            r'ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆ',
            r'ã®ç”³è«‹æ›¸ã‚’æ›¸ã„ã¦',
            r'ç”³è«‹æ›¸ã‚’æ›¸ã„ã¦',
            r'ã‚’æ›¸ã„ã¦',
            r'ã«ã¤ã„ã¦èª¿ã¹ã¦',
            r'ã«ã¤ã„ã¦è©³ã—ã',
            r'ã‚’èª¿ã¹ã¦',
            r'ã®è©³ç´°',
        ]
        
        sanitized = grant_name
        for phrase in remove_phrases:
            sanitized = re.sub(phrase, '', sanitized)
        
        # å‰å¾Œã®ç©ºç™½ã‚’é™¤å»
        sanitized = sanitized.strip()
        
        # æ‹¬å¼§å†…ã®å¹´åº¦æƒ…å ±ã¯ä¿æŒï¼ˆä¾‹ï¼šã€Œ2025å¹´åº¦å¾ŒæœŸã€ï¼‰
        # ãŸã ã—ã€ç©ºã«ãªã£ãŸå ´åˆã¯å…ƒã®åå‰ã‚’è¿”ã™
        if not sanitized:
            # æœ€ä½é™ã®ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼šæ˜ã‚‰ã‹ãªã‚³ãƒãƒ³ãƒ‰éƒ¨åˆ†ã®ã¿é™¤å»
            sanitized = grant_name.replace('ã®ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ä½œæˆã—ã¦', '').replace('ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ä½œæˆã—ã¦', '').strip()
        
        return sanitized

    def _research_grant_format(self, grant_name: str, user_id: str, grant_url: str = None) -> Tuple[str, List[Tuple[str, str]]]:
        """
        Researches the grant application format using Google Search Grounding.
        Also attempts to find and download application format files.
        
        Args:
            grant_name: Name of the grant to research
            user_id: User ID for file organization
            grant_url: Optional URL of the grant page (from Observer)
            
        Returns:
            Tuple of (format_info, downloaded_files)
            - format_info: Application format information text
            - downloaded_files: List of (file_path, filename) tuples
        """
        import logging
        from google.genai.types import GenerateContentConfig, Tool, GoogleSearch
        
        logging.info(f"[DRAFTER] Researching format for: {grant_name}, URL: {grant_url}")
        
        # If we have a URL from Observer, try Playwright scraping first
        if grant_url and grant_url != 'N/A':
            try:
                logging.info(f"[DRAFTER] Using provided URL for direct scraping: {grant_url}")
                playwright_files = self._scrape_url_for_files(grant_url, user_id)
                if playwright_files:
                    logging.info(f"[DRAFTER] Found {len(playwright_files)} files from provided URL")
                    format_info = f"""
## ç”³è«‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæƒ…å ±

å…¬å¼ãƒšãƒ¼ã‚¸ ({grant_url}) ã‹ã‚‰ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚

è©³ç´°ãªç”³è«‹æ–¹æ³•ã¯æ·»ä»˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚
"""
                    return (format_info, playwright_files)
            except Exception as e:
                logging.warning(f"[DRAFTER] Direct URL scraping failed: {e}")
        
        research_prompt = f"""
ä»¥ä¸‹ã®åŠ©æˆé‡‘ã®ç”³è«‹æ›¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆè³ªå•é …ç›®ãƒ»è¨˜å…¥æ¬„ï¼‰ã‚’èª¿æŸ»ã—ã¦ãã ã•ã„ã€‚

åŠ©æˆé‡‘å: {grant_name}

èª¿æŸ»ã™ã¹ãå†…å®¹:
1. ç”³è«‹æ›¸ã®è³ªå•é …ç›®ï¼ˆä¾‹ï¼šå›£ä½“æ¦‚è¦ã€äº‹æ¥­è¨ˆç”»ã€äºˆç®—ãªã©ï¼‰
2. å„é …ç›®ã®æ–‡å­—æ•°åˆ¶é™ã‚„è¨˜å…¥ä¾‹
3. å¯©æŸ»ã®ãƒã‚¤ãƒ³ãƒˆãƒ»è©•ä¾¡åŸºæº–
4. å¿…è¦ãªæ·»ä»˜æ›¸é¡
5. **é‡è¦**: ç”³è«‹æ›¸ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆPDFã€Wordã€Excelãªã©ï¼‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLãŒã‚ã‚Œã°ç‰¹å®šã—ã¦ãã ã•ã„

å‡ºåŠ›å½¢å¼:
## ç”³è«‹æ›¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

### è³ªå•é …ç›®
1. [é …ç›®å] ï¼ˆæ–‡å­—æ•°åˆ¶é™ãŒã‚ã‚Œã°è¨˜è¼‰ï¼‰
2. [é …ç›®å] ï¼ˆæ–‡å­—æ•°åˆ¶é™ãŒã‚ã‚Œã°è¨˜è¼‰ï¼‰
...

### å¯©æŸ»ãƒã‚¤ãƒ³ãƒˆ
- [ãƒã‚¤ãƒ³ãƒˆ1]
- [ãƒã‚¤ãƒ³ãƒˆ2]

### å¿…è¦æ›¸é¡
- [æ›¸é¡1]
- [æ›¸é¡2]

### ç”³è«‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«
- URL: [ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URL]ï¼ˆè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã¿è¨˜è¼‰ï¼‰
- ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: [PDF/Word/Excelç­‰]

â€»è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ä¸€èˆ¬çš„ãªåŠ©æˆé‡‘ç”³è«‹æ›¸ã®å½¢å¼ã‚’æƒ³å®šã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            # Use Google Search Grounding for format research
            google_search_tool = Tool(google_search=GoogleSearch())
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=research_prompt,
                config=GenerateContentConfig(
                    tools=[google_search_tool],
                    temperature=0.3
                )
            )
            
            
            format_info = response.text
            logging.info(f"[DRAFTER] Format research completed, length: {len(format_info)} chars")
            
            # 1. Extract file URLs from the text response
            import re
            url_pattern = r'https?://[^\s<>"\)]+\.(?:pdf|doc|docx|xls|xlsx|zip)'
            found_urls = set(re.findall(url_pattern, format_info, re.IGNORECASE))
            
            # 2. Extract page URLs from Grounding Metadata and deep search
            # Only explore URLs that appear to be related to the target grant
            try:
                if response.candidates and response.candidates[0].grounding_metadata:
                    metadata = response.candidates[0].grounding_metadata
                    if hasattr(metadata, 'grounding_chunks') and metadata.grounding_chunks:
                        # Extract keywords from grant name for URL validation
                        grant_keywords = self._extract_grant_keywords_for_validation(grant_name)
                        logging.info(f"[DRAFTER] Grant keywords for URL validation: {grant_keywords}")
                        
                        for chunk in metadata.grounding_chunks:
                            if hasattr(chunk, 'web') and chunk.web and chunk.web.uri:
                                page_url = chunk.web.uri
                                
                                # Resolve redirect if needed (simple version)
                                if 'grounding-api-redirect' in page_url:
                                    try:
                                        import requests
                                        res = requests.head(page_url, allow_redirects=True, timeout=5)
                                        page_url = res.url
                                    except:
                                        pass
                                
                                # Validate URL relevance before exploring
                                if not self._is_url_relevant_to_grant(page_url, grant_keywords):
                                    logging.info(f"[DRAFTER] Skipping unrelated URL: {page_url}")
                                    continue
                                
                                logging.info(f"[DRAFTER] Deep searching relevant page for files: {page_url}")
                                page_files = self.file_downloader.find_files_in_page(page_url)
                                found_urls.update(page_files)
            except Exception as e:
                logging.error(f"[DRAFTER] Error in deep search: {e}")
            
            downloaded_files = []
            failed_urls = []
            
            if found_urls:
                # Convert set back to list and sort to prioritize generic names? No, just list.
                url_list = list(found_urls)
                logging.info(f"[DRAFTER] Found {len(url_list)} potential format file URLs")
                
                # Filter out likely irrelevant files (images, common assets) if extension check failed
                # But regex ensures extension is valid.
                
                for url in url_list[:5]:  # Limit to first 5 URLs (increased from 3)
                    logging.info(f"[DRAFTER] Attempting to download: {url}")
                    result = self.file_downloader.download_file(url, user_id)
                    if result:
                        downloaded_files.append(result)
                        logging.info(f"[DRAFTER] Successfully downloaded: {result[1]}")
                    else:
                        failed_urls.append(url)
                        logging.warning(f"[DRAFTER] Failed to download: {url}")
                
                # Add download summary to format_info
                if downloaded_files or failed_urls:
                    summary = "\n\n---\n## ğŸ“ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ\n\n"
                    if downloaded_files:
                        summary += f"âœ… **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ**: {len(downloaded_files)}ä»¶\n"
                        for file_path, filename in downloaded_files:
                            summary += f"  - {filename}\n"
                    if failed_urls:
                        summary += f"\nâš ï¸ **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—**: {len(failed_urls)}ä»¶\n"
                        summary += "  ï¼ˆURLãŒç„¡åŠ¹ã€ã¾ãŸã¯ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸï¼‰\n"
                    format_info += summary
            else:
                logging.info("[DRAFTER] No format file URLs found in search results, trying Playwright deep search...")
                
                # Fallback: Use Playwright for deep search
                try:
                    playwright_files = self._run_playwright_deep_search(grant_name, user_id)
                    if playwright_files:
                        downloaded_files.extend(playwright_files)
                        summary = "\n\n---\n## ğŸ“ Playwrightæ·±æ˜ã‚Šæ¤œç´¢ã®çµæœ\n\n"
                        summary += f"âœ… **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ**: {len(playwright_files)}ä»¶\n"
                        for file_path, filename in playwright_files:
                            summary += f"  - {filename}\n"
                        format_info += summary
                    else:
                        logging.info("[DRAFTER] Playwright deep search also found no files")
                except Exception as pw_error:
                    logging.warning(f"[DRAFTER] Playwright deep search failed: {pw_error}")
            
            return (format_info, downloaded_files)
            
        except Exception as e:
            logging.error(f"[DRAFTER] Format research failed: {e}")
            # Return generic format as fallback
            return ("""
## ç”³è«‹æ›¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆä¸€èˆ¬çš„ãªå½¢å¼ï¼‰

### è³ªå•é …ç›®
1. å›£ä½“æ¦‚è¦ï¼ˆ400å­—ç¨‹åº¦ï¼‰
2. äº‹æ¥­ã®ç›®çš„ã¨èƒŒæ™¯ï¼ˆ600å­—ç¨‹åº¦ï¼‰
3. å…·ä½“çš„ãªæ´»å‹•è¨ˆç”»
4. æœŸå¾…ã•ã‚Œã‚‹æˆæœãƒ»åŠ¹æœ
5. äºˆç®—è¨ˆç”»
6. ä»Šå¾Œã®å±•æœ›

### å¯©æŸ»ãƒã‚¤ãƒ³ãƒˆ
- ç¤¾ä¼šçš„æ„ç¾©ã¨å¿…è¦æ€§
- å®Ÿç¾å¯èƒ½æ€§
- å›£ä½“ã®å®Ÿç¸¾ã¨ä¿¡é ¼æ€§
- è²»ç”¨å¯¾åŠ¹æœ
""", [])

    def _extract_grant_keywords_for_validation(self, grant_name: str) -> List[str]:
        """
        åŠ©æˆé‡‘åã‹ã‚‰URLæ¤œè¨¼ç”¨ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã™ã‚‹ã€‚
        çµ„ç¹”åã‚„åŠ©æˆé‡‘åã®ä¸»è¦éƒ¨åˆ†ã‚’æ„å‘³ã®ã‚ã‚‹å˜ä½ã§æŠ½å‡ºã™ã‚‹ã€‚
        """
        import re
        keywords = []
        
        if not grant_name:
            return keywords
        
        # è²¡å›£åãƒ»æ³•äººåã®æŠ½å‡º
        org_patterns = [
            r'(?:å…¬ç›Š)?(?:ç¤¾å›£|è²¡å›£)æ³•äºº\s*([^\sï¼ˆ(]+)',  # æ³•äººåï¼ˆæ‹¬å¼§å‰ã¾ã§ï¼‰
            r'([^\sãƒ»ï¼ˆ(]+è²¡å›£)',      # ï½è²¡å›£
            r'([^\sãƒ»ï¼ˆ(]+åŸºé‡‘)',      # ï½åŸºé‡‘
            r'([^\sãƒ»ï¼ˆ(]+å”ä¼š)',      # ï½å”ä¼š
            r'([^\sãƒ»ï¼ˆ(]+æ©Ÿæ§‹)',      # ï½æ©Ÿæ§‹
        ]
        
        for pattern in org_patterns:
            match = re.search(pattern, grant_name)
            if match:
                keywords.append(match.group(1))
        
        # ä¸­é»’ï¼ˆãƒ»ï¼‰ã‚„ã‚¹ãƒšãƒ¼ã‚¹ã§åŒºåˆ‡ã‚‰ã‚ŒãŸä¸»è¦ãªéƒ¨åˆ†ã‚’æŠ½å‡º
        # ä¾‹: "ã‚³ãƒ³ã‚µãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ãƒ»ã‚¢ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ãƒ»ã‚¸ãƒ£ãƒ‘ãƒ³" â†’ ["ã‚³ãƒ³ã‚µãƒ™ãƒ¼ã‚·ãƒ§ãƒ³", "ã‚¢ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹", "ã‚¸ãƒ£ãƒ‘ãƒ³"]
        parts = re.split(r'[ãƒ»\sã€€]+', grant_name)
        for part in parts:
            # æ‹¬å¼§ã‚„å¹´åº¦ã‚’é™¤å»
            clean_part = re.sub(r'[ï¼ˆ(ã€ã€Œ].*$', '', part)
            # 3æ–‡å­—ä»¥ä¸Šã®éƒ¨åˆ†ã®ã¿è¿½åŠ 
            if len(clean_part) >= 3:
                keywords.append(clean_part)
        
        # é‡è¤‡ã‚’é™¤å»ã—ã¦è¿”ã™
        return list(set(keywords))
    
    def _is_url_relevant_to_grant(self, url: str, grant_keywords: List[str]) -> bool:
        """
        URLãŒå¯¾è±¡åŠ©æˆé‡‘ã«é–¢é€£ã—ã¦ã„ã‚‹ã‹åˆ¤å®šã™ã‚‹ã€‚
        ãƒ‰ãƒ¡ã‚¤ãƒ³åˆ¤å®šã¯FQDNï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³åã®ã¿ï¼‰ã§è¡Œã†ã€‚
        
        Returns:
            True if URL appears to be related to the grant
        """
        from urllib.parse import urlparse
        
        url_lower = url.lower()
        
        # URLã‹ã‚‰FQDNï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³åï¼‰ã‚’æŠ½å‡º
        try:
            parsed = urlparse(url)
            fqdn = parsed.netloc.lower()  # ä¾‹: "outdoorconservation.jp"
            path = parsed.path.lower()    # ä¾‹: "/promotion-support"
        except Exception:
            fqdn = ""
            path = ""
        
        logging.debug(f"[DRAFTER] URL validation: FQDN={fqdn}, path={path}")
        
        # Googleæ¤œç´¢ãƒšãƒ¼ã‚¸ã¯é™¤å¤–ï¼ˆgrounding_metadataãŒæ¤œç´¢ãƒšãƒ¼ã‚¸ã‚’è¿”ã™ã“ã¨ãŒã‚ã‚‹ï¼‰
        if 'google.com' in fqdn or 'google.co.jp' in fqdn:
            if '/search' in path:
                logging.info(f"[DRAFTER] Blocking Google search page URL: {url[:100]}")
                return False
        
        # æ˜ã‚‰ã‹ã«ç„¡é–¢ä¿‚ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’é™¤å¤–ï¼ˆFQDNã§åˆ¤å®šï¼‰
        blocked_domains = [
            'amazon.co.jp', 'amazon.com', 'rakuten.co.jp',
            'yahoo.co.jp',  # YahooçŸ¥æµè¢‹ãªã©
            'twitter.com', 'x.com', 'facebook.com', 'instagram.com',
            'youtube.com', 'wikipedia.org',
            'note.com',  # å€‹äººãƒ–ãƒ­ã‚°
            'time.is',  # æ™‚åˆ»ã‚µã‚¤ãƒˆ
            'weather.com', 'tenki.jp',  # å¤©æ°—ã‚µã‚¤ãƒˆ
        ]
        for blocked in blocked_domains:
            if fqdn == blocked or fqdn.endswith('.' + blocked):
                return False
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚Œã°URLå…¨ä½“ã§è¨±å¯
        if grant_keywords:
            for keyword in grant_keywords:
                if keyword.lower() in url_lower:
                    return True
        
        # ä¿¡é ¼ã§ãã‚‹æ—¥æœ¬ã®ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆFQDNã§åˆ¤å®šï¼‰
        # .jp ã§çµ‚ã‚ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³ã¯åŸºæœ¬çš„ã«ä¿¡é ¼
        if fqdn.endswith('.jp'):
            return True
        
        # ãã®ä»–ã®ä¿¡é ¼ãƒ‰ãƒ¡ã‚¤ãƒ³
        trusted_tlds = ['.org', '.edu', '.gov']
        for tld in trusted_tlds:
            if fqdn.endswith(tld):
                return True
        
        # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‚’å«ã‚€URLã¯è¨±å¯ï¼ˆç›´æ¥ãƒ•ã‚¡ã‚¤ãƒ«ãƒªãƒ³ã‚¯ï¼‰
        if any(ext in path for ext in ['.pdf', '.docx', '.doc', '.xlsx', '.xls', '.zip']):
            return True
        
        # åŠ©æˆé‡‘é–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ãƒ‘ã‚¹ã«å«ã‚€URLã¯è¨±å¯
        grant_related_keywords = [
            'åŠ©æˆ', 'è£œåŠ©', 'æ”¯æ´', 'ç”³è«‹', 'å…¬å‹Ÿ', 'å‹Ÿé›†',  # æ—¥æœ¬èª
            'grant', 'subsidy', 'application', 'support', 'promotion', 'fund'  # è‹±èª
        ]
        for kw in grant_related_keywords:
            if kw in path:
                return True
        
        # ãã‚Œä»¥å¤–ã¯ç„¡é–¢ä¿‚ã¨åˆ¤æ–­
        logging.info(f"[DRAFTER] URL does not match criteria, skipping: {url[:100]} (FQDN: {fqdn})")
        return False

    def _analyze_application_format(
        self, 
        format_files: List[Tuple[str, str]], 
        grant_name: str
    ) -> str:
        """
        ç”³è«‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’Gemini 3.0 Proã§è§£æã—ã€
        è³ªå•é …ç›®ãƒ»è¨˜å…¥æ¬„ãƒ»æ–‡å­—æ•°åˆ¶é™ãªã©ã‚’æŠ½å‡ºã™ã‚‹ã€‚
        
        Args:
            format_files: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ[(file_path, filename), ...]
            grant_name: åŠ©æˆé‡‘å
            
        Returns:
            è§£æçµæœã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆè³ªå•é …ç›®ã€æ–‡å­—æ•°åˆ¶é™ã€è¨˜å…¥ã®ãƒã‚¤ãƒ³ãƒˆãªã©ï¼‰
        """
        if not format_files:
            logging.info("[DRAFTER] No format files to analyze")
            return ""
        
        logging.info(f"[DRAFTER] Analyzing {len(format_files)} format files with Gemini")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’åé›†
        file_contents_text = ""
        analyzed_count = 0
        
        for file_path, filename in format_files[:5]:  # æœ€å¤§5ãƒ•ã‚¡ã‚¤ãƒ«ã¾ã§
            try:
                file_ext = filename.lower().split('.')[-1] if '.' in filename else ''
                
                # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
                if file_ext == 'pdf':
                    content = self._extract_pdf_content(file_path)
                    if content:
                        file_contents_text += f"\n\n---\n### ãƒ•ã‚¡ã‚¤ãƒ«: {filename}\n{content[:8000]}\n"
                        analyzed_count += 1
                        
                # Word/Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
                elif file_ext in ['doc', 'docx', 'xls', 'xlsx']:
                    content = self._extract_office_content(file_path, file_ext)
                    if content:
                        file_contents_text += f"\n\n---\n### ãƒ•ã‚¡ã‚¤ãƒ«: {filename}\n{content[:8000]}\n"
                        analyzed_count += 1
                        
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
                elif file_ext in ['txt', 'text']:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()[:8000]
                        file_contents_text += f"\n\n---\n### ãƒ•ã‚¡ã‚¤ãƒ«: {filename}\n{content}\n"
                        analyzed_count += 1
                        
            except Exception as e:
                logging.warning(f"[DRAFTER] Error reading file {filename}: {e}")
                continue
        
        if not file_contents_text or analyzed_count == 0:
            logging.info("[DRAFTER] Could not extract content from any files")
            return ""
        
        logging.info(f"[DRAFTER] Successfully extracted content from {analyzed_count} files")
        
        # Gemini 3.0 Proã§ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’è§£æ
        try:
            format_analyzer_prompt = self.config.get("system_prompts", {}).get("format_analyzer", "")
            if not format_analyzer_prompt:
                logging.warning("[DRAFTER] format_analyzer prompt not found in config")
                return ""
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’åŸ‹ã‚è¾¼ã¿
            full_prompt = format_analyzer_prompt.replace("{file_contents}", file_contents_text)
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=full_prompt
            )
            
            analysis_result = response.text
            logging.info(f"[DRAFTER] Format analysis completed, length: {len(analysis_result)} chars")
            
            return analysis_result
            
        except Exception as e:
            logging.error(f"[DRAFTER] Format analysis failed: {e}")
            return ""
    
    def _extract_pdf_content(self, file_path: str) -> str:
        """PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹"""
        try:
            import fitz  # PyMuPDF
            
            doc = fitz.open(file_path)
            text_content = ""
            
            for page_num in range(min(doc.page_count, 10)):  # æœ€å¤§10ãƒšãƒ¼ã‚¸ã¾ã§
                page = doc[page_num]
                text_content += page.get_text() + "\n"
            
            doc.close()
            return text_content.strip()
            
        except ImportError:
            logging.warning("[DRAFTER] PyMuPDF (fitz) not installed, skipping PDF extraction")
            return ""
        except Exception as e:
            logging.warning(f"[DRAFTER] PDF extraction failed: {e}")
            return ""
    
    def _extract_office_content(self, file_path: str, file_ext: str) -> str:
        """Word/Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹"""
        try:
            if file_ext in ['doc', 'docx']:
                from docx import Document
                doc = Document(file_path)
                return "\n".join([para.text for para in doc.paragraphs])
                
            elif file_ext in ['xls', 'xlsx']:
                import openpyxl
                wb = openpyxl.load_workbook(file_path, read_only=True)
                text_content = ""
                
                for sheet in wb.worksheets[:3]:  # æœ€å¤§3ã‚·ãƒ¼ãƒˆã¾ã§
                    for row in sheet.iter_rows(max_row=100):  # æœ€å¤§100è¡Œã¾ã§
                        row_text = " | ".join([str(cell.value) if cell.value else "" for cell in row])
                        if row_text.strip():
                            text_content += row_text + "\n"
                
                return text_content.strip()
                
        except ImportError as e:
            logging.warning(f"[DRAFTER] Office library not installed: {e}")
            return ""
        except Exception as e:
            logging.warning(f"[DRAFTER] Office file extraction failed: {e}")
            return ""

    def _scrape_url_for_files(self, url: str, user_id: str) -> List[Tuple[str, str]]:

        """
        Scrape a specific URL for format files using Playwright.
        This is used when we have a verified URL from Observer.
        """
        try:
            import nest_asyncio
            nest_asyncio.apply()
            return asyncio.run(self._async_scrape_url_for_files(url, user_id))
        except Exception as e:
            logging.error(f"[DRAFTER] _scrape_url_for_files error: {e}")
            return []
    
    async def _async_scrape_url_for_files(self, url: str, user_id: str) -> List[Tuple[str, str]]:
        """
        Async method to scrape a URL for format files.
        """
        downloaded_files = []
        
        try:
            # Use page scraper to get grant info and files
            grant_info = await self.page_scraper.find_grant_info(url, "")
            
            if not grant_info.get('accessible'):
                logging.warning(f"[DRAFTER] Page not accessible: {url}")
                return []
            
            format_files = grant_info.get('format_files', [])
            logging.info(f"[DRAFTER] Found {len(format_files)} format files on page")
            
            # Download files
            for file_info in format_files[:5]:
                file_url = file_info.get('url')
                if not file_url:
                    continue
                
                logging.info(f"[DRAFTER] Downloading: {file_url}")
                result = self.file_downloader.download_file(file_url, user_id)
                if result:
                    downloaded_files.append(result)
                    logging.info(f"[DRAFTER] Downloaded: {result[1]}")
                    
        except Exception as e:
            logging.error(f"[DRAFTER] Async scrape error: {e}")
        
        return downloaded_files

    def _run_playwright_deep_search(self, grant_name: str, user_id: str) -> List[Tuple[str, str]]:
        """
        Run Playwright-based deep search for format files.
        Uses nest_asyncio to allow running async code within Discord.py's event loop.
        """
        try:
            import nest_asyncio
            nest_asyncio.apply()
            
            # Extract organization name for targeted search
            from src.logic.grant_validator import GrantValidator
            validator = GrantValidator()
            org_name = validator.extract_organization_name(grant_name)
            
            if not org_name:
                logging.info("[DRAFTER] Could not extract organization name for Playwright search")
                return []
            
            # Sanitize grant_name: remove command-like phrases that shouldn't be in search
            sanitized_grant_name = self._sanitize_grant_name_for_search(grant_name)
            
            # Build more specific search URL with grant name for better targeting
            # Include both organization name and grant name for more precise results
            search_query = f'"{org_name}" "{sanitized_grant_name}" ç”³è«‹æ›¸ æ§˜å¼ filetype:pdf OR filetype:docx OR filetype:xlsx'
            search_url = f"https://www.google.com/search?q={search_query.replace(' ', '+')}"
            
            logging.info(f"[DRAFTER] Playwright deep search for: {grant_name} (org: {org_name})")
            
            # Now we can safely run asyncio.run() within the existing event loop
            return asyncio.run(self._async_playwright_deep_search(search_url, grant_name, user_id))
            
        except Exception as e:
            logging.error(f"[DRAFTER] Playwright deep search error: {e}")
            return []
    
    async def _async_playwright_deep_search(
        self, 
        start_url: str, 
        grant_name: str, 
        user_id: str
    ) -> List[Tuple[str, str]]:
        """
        Async Playwright deep search for format files.
        """
        downloaded_files = []
        
        try:
            # Use deep search to find format files (pass grant_name for relevance filtering)
            format_files = await self.page_scraper.deep_search_format_files(start_url, max_depth=2, grant_name=grant_name)
            
            if not format_files:
                logging.info("[DRAFTER] Playwright found no format files")
                return []
            
            logging.info(f"[DRAFTER] Playwright found {len(format_files)} potential files")
            
            # Download top-scored files
            for file_info in format_files[:5]:
                file_url = file_info.get('url')
                if not file_url:
                    continue
                
                logging.info(f"[DRAFTER] Downloading: {file_url}")
                result = self.file_downloader.download_file(file_url, user_id)
                if result:
                    downloaded_files.append(result)
                    logging.info(f"[DRAFTER] Downloaded: {result[1]}")
                    
        except Exception as e:
            logging.error(f"[DRAFTER] Async deep search error: {e}")
        
        return downloaded_files

    def create_draft(self, user_id: str, grant_info: str) -> tuple[str, str, str, List[Tuple[str, str]]]:
        """
        Generates a grant application draft based on researched format with progress notifications.
        
        Returns:
            tuple: (message, draft_content, filename, format_files)
            - format_files: List of (file_path, filename) tuples for downloaded files
        """
        import logging
        from src.utils.progress_notifier import get_progress_notifier, ProgressStage
        
        logging.info(f"[DRAFTER] create_draft started for user: {user_id}")
        notifier = get_progress_notifier()
        
        pm = ProfileManager(user_id=user_id)
        profile = pm.get_profile_context()
        
        logging.info(f"[DRAFTER] Profile loaded, length: {len(profile)} chars")
        
        # Extract grant name and URL from grant_info
        grant_name = grant_info.strip()
        grant_url = None
        
        # Try to extract URL from grant_info
        import re
        url_match = re.search(r'URL:\s*(https?://[^\s]+)', grant_info)
        if url_match:
            grant_url = url_match.group(1).strip()
            logging.info(f"[DRAFTER] Extracted URL from grant_info: {grant_url}")
        
        # Try to extract just the grant name if it contains other info
        name_match = re.search(r'åŠ©æˆé‡‘å:\s*(.+?)(?:\n|$)', grant_info)
        if name_match:
            grant_name = name_match.group(1).strip()
        elif "åŠ©æˆ" in grant_name:
            # Find the grant name pattern
            match = re.search(r'[^\s]+åŠ©æˆ[^\s]*', grant_name)
            if match:
                grant_name = match.group(0)
        
        # Create display name (max 20 chars) for notifications
        grant_display_name = grant_name[:20] + "..." if len(grant_name) > 20 else grant_name
        
        logging.info(f"[DRAFTER] Grant name: {grant_name}, URL: {grant_url}")
        
        # Start notification
        notifier.notify_sync(
            ProgressStage.STARTING,
            f"âœ¨ [{grant_display_name}] ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆã‚’é–‹å§‹ã—ã¾ã™..."
        )
        
        # Step 1: Research the application format (prioritize URL if available)
        logging.info(f"[DRAFTER] Step 1: Researching format for '{grant_name}'")
        format_info, format_files = self._research_grant_format(grant_name, user_id, grant_url=grant_url)
        
        # Step 1.5: Early file classification to filter irrelevant files
        # This prevents wasting resources on analyzing unrelated files
        related_files = []
        excluded_files = []
        if format_files:
            logging.info(f"[DRAFTER] Step 1.5: Classifying {len(format_files)} files for relevance")
            notifier.notify_sync(
                ProgressStage.PROCESSING,
                f"ğŸ” [{grant_display_name}] ãƒ•ã‚¡ã‚¤ãƒ«ã®é–¢é€£æ€§ã‚’ç¢ºèªä¸­..."
            )
            
            for file_path, file_name in format_files:
                file_type = self.file_classifier.classify_format_file(file_name, file_path, grant_name)
                
                # Check if file is unrelated (contains "åˆ¥ã®åŠ©æˆé‡‘ã®å¯èƒ½æ€§")
                if "åˆ¥ã®åŠ©æˆé‡‘ã®å¯èƒ½æ€§" in file_type:
                    excluded_files.append((file_path, file_name, file_type))
                    logging.info(f"[DRAFTER] Excluding unrelated file: {file_name}")
                else:
                    related_files.append((file_path, file_name, file_type))
                    logging.info(f"[DRAFTER] Related file: {file_name} -> {file_type}")
            
            logging.info(f"[DRAFTER] File classification complete: {len(related_files)} related, {len(excluded_files)} excluded")
            
            # Update format_files to only include related files (without file_type for compatibility)
            format_files = [(fp, fn) for fp, fn, _ in related_files]
        
        # Step 2: Analyze downloaded format files with Gemini 3.0 Pro (only related files)
        format_analysis = ""
        if format_files:
            logging.info(f"[DRAFTER] Step 2: Analyzing {len(format_files)} format files with Gemini")
            notifier.notify_sync(
                ProgressStage.PROCESSING,
                f"ğŸ“‹ [{grant_display_name}] ç”³è«‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’è§£æä¸­..."
            )
            format_analysis = self._analyze_application_format(format_files, grant_name)
            if format_analysis:
                logging.info(f"[DRAFTER] Format analysis completed, length: {len(format_analysis)} chars")
            else:
                logging.info("[DRAFTER] Format analysis returned empty, using format_info only")
        
        # Step 3: Generate draft based on format and analysis
        logging.info(f"[DRAFTER] Step 3: Generating format-aware draft")
        
        notifier.notify_sync(
            ProgressStage.PROCESSING,
            f"ğŸ“ [{grant_display_name}] ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ç”Ÿæˆä¸­..."
        )
        
        # Combine format_info and format_analysis for comprehensive context
        combined_format_info = format_info
        if format_analysis:
            combined_format_info += f"\n\n---\n\n# ç”³è«‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè©³ç´°è§£æçµæœ\n{format_analysis}"
        
        full_prompt = f"""
{self.system_prompt}

# Soul Profileï¼ˆé­‚ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
{profile}

# å¯¾è±¡åŠ©æˆé‡‘
{grant_info}

# ç”³è«‹æ›¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæƒ…å ±
{combined_format_info}

# ã‚¿ã‚¹ã‚¯
ä¸Šè¨˜ã®ç”³è«‹æ›¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆç‰¹ã«ã€Œç”³è«‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè©³ç´°è§£æçµæœã€ã®è³ªå•é …ç›®ï¼‰ã«å¾“ã£ã¦ã€å„è³ªå•é …ç›®ã«å¯¾ã™ã‚‹å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**é‡è¦ãªæŒ‡ç¤º:**
1. è§£æçµæœã®è³ªå•é …ç›®ä¸€è¦§ã«è¨˜è¼‰ã•ã‚ŒãŸé …ç›®ã”ã¨ã«è¦‹å‡ºã—ã‚’ä»˜ã‘ã¦å›ç­”ã‚’ä½œæˆ
2. æ–‡å­—æ•°åˆ¶é™ãŒæ˜è¨˜ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã«åã¾ã‚‹ã‚ˆã†ã«èª¿æ•´
3. Soul Profileã®æƒ…å ±ã‚’æœ€å¤§é™æ´»ç”¨
4. å„å›ç­”ã®å¾Œã«ç°¡å˜ãªğŸ“è¨˜å…¥ã®ãƒã‚¤ãƒ³ãƒˆã‚’è¿½è¨˜
5. å¯©æŸ»ã§é‡è¦–ã•ã‚Œã‚‹ç‚¹ã‚’æ„è­˜ã—ã¦å›ç­”ã‚’ä½œæˆ

**å‡ºåŠ›å½¢å¼:**
# [åŠ©æˆé‡‘å] ç”³è«‹æ›¸ãƒ‰ãƒ©ãƒ•ãƒˆ

## 1. [è³ªå•é …ç›®1]
[å›ç­”å†…å®¹]
ğŸ“ ãƒã‚¤ãƒ³ãƒˆ: [ã“ã®é …ç›®ã§å¼·èª¿ã™ã¹ãç‚¹]

## 2. [è³ªå•é …ç›®2]
[å›ç­”å†…å®¹]
ğŸ“ ãƒã‚¤ãƒ³ãƒˆ: [ã“ã®é …ç›®ã§å¼·èª¿ã™ã¹ãç‚¹]

...

---
## ğŸ“‹ å…¨ä½“ã®è€ƒæ…®ç‚¹
[ç”³è«‹å…¨ä½“ã§æ°—ã‚’ã¤ã‘ã‚‹ã¹ãç‚¹]

## ğŸŒŸ ã‚¢ãƒ”ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒˆ
[ç‰¹ã«å¼·èª¿ã™ã¹ãå›£ä½“ã®å¼·ã¿]

## âš ï¸ æ‡¸å¿µç‚¹ãƒ»æ”¹å–„ææ¡ˆ
[ç”³è«‹ã§å¼±ããªã‚Šãã†ãªç‚¹ã¨å¯¾ç­–]
"""
        try:
            logging.info(f"[DRAFTER] Calling Gemini model: {self.model_name}")
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=full_prompt
            )
            draft_content = response.text
            logging.info(f"[DRAFTER] Draft generated, length: {len(draft_content)} chars")
            
            # Extract a title (first line or generic)
            lines = draft_content.split('\n')
            title = "Grant_Draft"
            if lines and lines[0].startswith('# '):
                 title = lines[0].replace('# ', '').strip()
            
            logging.info(f"[DRAFTER] Title: {title}")
            
            notifier.notify_sync(
                ProgressStage.PROCESSING,
                f"ğŸ’¾ [{grant_display_name}] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä¿å­˜ä¸­..."
            )
            
            file_path = self.docs_tool.create_document(title, draft_content, user_id=user_id)
            logging.info(f"[DRAFTER] Document saved: {file_path}")
            
            # Extract filename from path
            import os
            if 'gs://' in file_path:
                # GCS path: gs://bucket/drafts/user_id/filename.md
                filename = file_path.split('/')[-1]
            elif 'Google Doc' in file_path:
                # Google Docs: extract from message
                filename = f"{title}.md"
            else:
                # Local path
                filename = os.path.basename(file_path)
            
            logging.info(f"[DRAFTER] Filename: {filename}")
            
            message = f"ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸ: {file_path}"
            
            # Step 4: Fill format files with draft content (field-by-field processing)
            filled_files = []
            if format_files:
                logging.info(f"[DRAFTER] Step 4: Attempting to fill {len(format_files)} format files (field-by-field)")
                notifier.notify_sync(
                    ProgressStage.PROCESSING,
                    f"ğŸ“ [{grant_display_name}] ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«é …ç›®åˆ¥ã«å…¥åŠ›ä¸­..."
                )
                
                # å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã®field_valuesã‚’è“„ç©ï¼ˆæ‡¸å¿µç‚¹ãƒ¬ãƒãƒ¼ãƒˆç”¨ï¼‰
                all_field_values = {}
                
                for file_path_orig, file_name_orig in format_files:
                    try:
                        # Only process Excel/Word files
                        ext = os.path.splitext(file_name_orig)[1].lower()
                        if ext not in ['.xlsx', '.xlsm', '.xls', '.docx', '.doc']:
                            logging.info(f"[DRAFTER] Skipping non-fillable file: {file_name_orig}")
                            continue
                        
                        # Analyze format fields using VLM
                        fields, file_type = self.format_mapper.analyze_format_file(file_path_orig)
                        
                        if not fields:
                            logging.info(f"[DRAFTER] No fillable fields found in: {file_name_orig}")
                            continue
                        
                        logging.info(f"[DRAFTER] Found {len(fields)} fields in {file_name_orig}")
                        
                        # Fill fields individually using profile (field-by-field processing)
                        # Note: No Discord notification per field - progress is logged only
                        field_values = self.format_mapper.fill_fields_individually(
                            fields=fields,
                            profile=profile,
                            grant_name=grant_name,
                            grant_info=grant_info
                        )
                        
                        if not field_values:
                            logging.info(f"[DRAFTER] Could not fill fields in: {file_name_orig}")
                            continue
                        
                        # æ‡¸å¿µç‚¹ãƒ¬ãƒãƒ¼ãƒˆç”¨ã«è“„ç©
                        all_field_values.update(field_values)
                        
                        # Fill the document
                        filled_path, fill_message = self.document_filler.fill_document(
                            file_path_orig,
                            field_values,
                            user_id
                        )
                        
                        if filled_path:
                            filled_filename = os.path.basename(filled_path)
                            filled_files.append((filled_path, filled_filename))
                            logging.info(f"[DRAFTER] Successfully filled: {filled_filename}")
                        else:
                            logging.warning(f"[DRAFTER] Fill failed for {file_name_orig}: {fill_message}")
                            
                    except Exception as fill_error:
                        logging.warning(f"[DRAFTER] Error filling {file_name_orig}: {fill_error}")
                
                if filled_files:
                    message += f"\nğŸ“‹ {len(filled_files)}ä»¶ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«é …ç›®åˆ¥å…¥åŠ›ã—ã¾ã—ãŸ"
                
                # æ‡¸å¿µç‚¹ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ
                if all_field_values:
                    concern_report = self.format_mapper.generate_concern_report(all_field_values)
                    if concern_report:
                        message += f"\n\n{concern_report}"
                        logging.info(f"[DRAFTER] Generated concern report")
                    
                    # äº‹å‹™å±€é•·ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç”Ÿæˆ
                    director_review = self._generate_director_review(
                        grant_name=grant_name,
                        field_values=all_field_values,
                        profile=profile
                    )
                    if director_review:
                        message += f"\n\n{director_review}"
                        logging.info(f"[DRAFTER] Generated director review")
            
            # Completion notification
            notifier.notify_sync(
                ProgressStage.COMPLETED,
                f"âœ… [{grant_display_name}] ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆå®Œäº†ï¼"
            )
            
            logging.info(f"[DRAFTER] create_draft completed successfully, filled_files: {len(filled_files)}")
            # Return related_files with file_type info (file_path, file_name, file_type)
            return (message, draft_content, filename, related_files, filled_files)
            
        except Exception as e:
            logging.error(f"[DRAFTER] Error in create_draft: {e}", exc_info=True)
            
            # Error notification
            notifier.notify_sync(
                ProgressStage.ERROR,
                f"âš ï¸ [{grant_display_name}] ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)[:30]}..."
            )
            
            error_msg = f"ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}"
            return (error_msg, "", "", [], [])


    def list_drafts(self, user_id: str) -> str:
        """
        Lists all drafts for a user.
        
        Args:
            user_id: User ID
            
        Returns:
            Formatted list of drafts or message if none found
        """
        try:
            drafts = self.docs_tool.list_drafts(user_id)
            
            if not drafts:
                return "ã¾ã ãƒ‰ãƒ©ãƒ•ãƒˆãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã€ŒåŠ©æˆé‡‘ç”³è«‹æ›¸ã‚’æ›¸ã„ã¦ã€ã¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚"
            
            result = f"ğŸ“„ **ä¿å­˜æ¸ˆã¿ãƒ‰ãƒ©ãƒ•ãƒˆä¸€è¦§** ({len(drafts)}ä»¶)\n\n"
            for i, filename in enumerate(drafts, 1):
                result += f"{i}. `{filename}`\n"
            
            result += "\nğŸ’¡ ç‰¹å®šã®ãƒ‰ãƒ©ãƒ•ãƒˆã‚’è¦‹ã‚‹ã«ã¯ã€Œ[ãƒ•ã‚¡ã‚¤ãƒ«å]ã‚’è¦‹ã›ã¦ã€ã¾ãŸã¯ã€Œæœ€æ–°ã®ãƒ‰ãƒ©ãƒ•ãƒˆã‚’è¦‹ã›ã¦ã€ã¨é€ä¿¡ã—ã¦ãã ã•ã„ã€‚"
            
            return result
            
        except Exception as e:
            return f"ãƒ‰ãƒ©ãƒ•ãƒˆä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}"

    def clear_drafts(self, user_id: str) -> str:
        """
        Clears all drafts for a user.
        
        Args:
            user_id: User ID
            
        Returns:
            Success message
        """
        try:
            return self.docs_tool.clear_drafts(user_id)
        except Exception as e:
            return f"ãƒ‰ãƒ©ãƒ•ãƒˆã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}"

    def get_latest_draft(self, user_id: str) -> tuple[str, Optional[str]]:
        """
        Gets the latest draft for a user.
        
        Args:
            user_id: User ID
            
        Returns:
            Tuple of (message, content). If content is present, it should be sent as attachment.
        """
        try:
            drafts = self.docs_tool.list_drafts(user_id)
            
            if not drafts:
                return ("ã¾ã ãƒ‰ãƒ©ãƒ•ãƒˆãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", None)
            
            # Sort by filename (which includes timestamp)
            latest_draft = sorted(drafts)[-1]
            content = self.docs_tool.get_draft(user_id, latest_draft)
            
            if not content:
                return (f"ãƒ‰ãƒ©ãƒ•ãƒˆ '{latest_draft}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", None)
            
            message = f"ğŸ“„ **æœ€æ–°ã®ãƒ‰ãƒ©ãƒ•ãƒˆ**: `{latest_draft}`\n\n"
            
            # If content is short, include it in message
            if len(content) <= 1800:
                message += f"```markdown\n{content}\n```"
                return (message, None)
            else:
                # Return content for file attachment
                message += "ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦é€ä¿¡ã—ã¾ã™ï¼‰"
                return (message, content)
                
        except Exception as e:
            return (f"æœ€æ–°ãƒ‰ãƒ©ãƒ•ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}", None)

    def get_draft(self, user_id: str, filename: str) -> tuple[str, Optional[str]]:
        """
        Gets a specific draft by filename.
        
        Args:
            user_id: User ID
            filename: Draft filename
            
        Returns:
            Tuple of (message, content). If content is present, it should be sent as attachment.
        """
        try:
            content = self.docs_tool.get_draft(user_id, filename)
            
            if not content:
                # Try fuzzy match
                drafts = self.docs_tool.list_drafts(user_id)
                matches = [d for d in drafts if filename.lower() in d.lower()]
                
                if matches:
                    if len(matches) == 1:
                        # Use the matched file
                        filename = matches[0]
                        content = self.docs_tool.get_draft(user_id, filename)
                    else:
                        suggestion = "\n\nå€™è£œ:\n" + "\n".join([f"- {m}" for m in matches])
                        return (f"ãƒ•ã‚¡ã‚¤ãƒ«åãŒæ›–æ˜§ã§ã™ã€‚{suggestion}", None)
                else:
                    return (f"ãƒ‰ãƒ©ãƒ•ãƒˆ '{filename}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€Œãƒ‰ãƒ©ãƒ•ãƒˆä¸€è¦§ã€ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚", None)
            
            message = f"ğŸ“„ **ãƒ‰ãƒ©ãƒ•ãƒˆ**: `{filename}`\n\n"
            
            # If content is short, include it in message
            if len(content) <= 1800:
                message += f"```markdown\n{content}\n```"
                return (message, None)
            else:
                # Return content for file attachment
                message += "ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦é€ä¿¡ã—ã¾ã™ï¼‰"
                return (message, content)
                
        except Exception as e:
            return (f"ãƒ‰ãƒ©ãƒ•ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}", None)

    def _generate_director_review(
        self, 
        grant_name: str, 
        field_values: Dict[str, Any],
        profile: str
    ) -> str:
        """
        äº‹å‹™å±€é•·ã®è¦³ç‚¹ã‹ã‚‰ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€ã‚³ãƒ¡ãƒ³ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚
        
        Args:
            grant_name: åŠ©æˆé‡‘å
            field_values: å…¥åŠ›ã•ã‚ŒãŸãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å€¤
            profile: NPOãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
            
        Returns:
            Markdownå½¢å¼ã®äº‹å‹™å±€é•·ãƒ¬ãƒ“ãƒ¥ãƒ¼
        """
        if not self.client or not field_values:
            return ""
        
        try:
            # ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰å€¤ã‚’ãƒ†ã‚­ã‚¹ãƒˆåŒ–
            fields_text = "\n".join([
                f"- **{data.get('field_name', fid)}**: {data.get('value', '')[:200]}..."
                if len(data.get('value', '')) > 200
                else f"- **{data.get('field_name', fid)}**: {data.get('value', '')}"
                for fid, data in field_values.items()
                if data.get('value')
            ])
            
            prompt = f"""ã‚ãªãŸã¯NPOã®äº‹å‹™å±€é•·ã¨ã—ã¦ã€åŠ©æˆé‡‘ç”³è«‹æ›¸ã®ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ãã ã•ã„ã€‚

# å¯¾è±¡åŠ©æˆé‡‘
{grant_name}

# NPOãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«æ¦‚è¦
{profile[:2000]}

# å…¥åŠ›ã•ã‚ŒãŸãƒ‰ãƒ©ãƒ•ãƒˆå†…å®¹
{fields_text[:4000]}

# ãƒ¬ãƒ“ãƒ¥ãƒ¼è¦³ç‚¹
1. **å…¨ä½“è©•ä¾¡**: ç”³è«‹æ›¸å…¨ä½“ã¨ã—ã¦ã®å®Œæˆåº¦ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„
2. **å¼·ã¿**: ã“ã®ç”³è«‹æ›¸ã®è‰¯ã„ç‚¹ã‚’æŒ™ã’ã¦ãã ã•ã„
3. **æ”¹å–„ææ¡ˆ**: ã‚ˆã‚Šèª¬å¾—åŠ›ã‚’é«˜ã‚ã‚‹ãŸã‚ã®å…·ä½“çš„ãªæ”¹å–„ææ¡ˆã‚’ã—ã¦ãã ã•ã„
4. **ç¢ºèªäº‹é …**: æå‡ºå‰ã«å›£ä½“å†…ã§ç¢ºèªã™ã¹ãäº‹é …ã‚’æŒ™ã’ã¦ãã ã•ã„

# å‡ºåŠ›å½¢å¼
äº‹å‹™å±€é•·ã¨ã—ã¦ã®ç°¡æ½”ãªã‚³ãƒ¡ãƒ³ãƒˆï¼ˆ300å­—ç¨‹åº¦ï¼‰ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
ç®‡æ¡æ›¸ãã§ã¯ãªãã€è‡ªç„¶ãªæ–‡ç« ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚
"""
            
            response = self.client.models.generate_content(
                model="gemini-3-flash-preview",
                contents=prompt
            )
            
            review_text = response.text.strip()
            
            if review_text:
                return f"## ğŸ“ äº‹å‹™å±€é•·ãƒ¬ãƒ“ãƒ¥ãƒ¼\n\n{review_text}"
            
            return ""
            
        except Exception as e:
            logging.warning(f"[DRAFTER] Director review generation failed: {e}")
            return ""
