from typing import Dict, Any, Optional, Tuple, List
import yaml
import os
import asyncio
import logging
from src.tools.gdocs_tool import GoogleDocsTool
from src.memory.profile_manager import ProfileManager
from src.tools.file_downloader import FileDownloader
from src.logic.grant_page_scraper import GrantPageScraper

class DrafterAgent:
    def __init__(self):
        self.config = self._load_config()
        self.system_prompt = self.config.get("system_prompts", {}).get("drafter", "")
        
        # Initialize Gemini client via Vertex AI backend
        try:
            from src.utils.gemini_client import get_gemini_client
            self.client = get_gemini_client()
            logging.info("[DRAFTER] Gemini client initialized via Vertex AI")
        except Exception as e:
            logging.error(f"[DRAFTER] Failed to init Gemini client: {e}")
            self.client = None
            
        # Using Interviewer model (Pro) for drafting as it requires high reasoning/writing capability
        self.model_name = self.config.get("model_config", {}).get("interviewer_model")
        if not self.model_name:
             raise ValueError("interviewer_model (for drafter) not found in config")
        self.docs_tool = GoogleDocsTool()
        self.file_downloader = FileDownloader()
        
        # Initialize page scraper with Gemini client for visual fallback
        # Use shorter timeout (10s) for drafter operations to avoid long hangs
        self.page_scraper = GrantPageScraper(
            gemini_client=self.client, 
            model_name=self.model_name,
            timeout=10000  # 10 seconds timeout for Playwright operations
        )

    def _load_config(self) -> Dict[str, Any]:
        try:
            with open("config/prompts.yaml", "r", encoding="utf-8") as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"Error loading config: {e}")
            return {}

    def _research_grant_format(self, grant_name: str, user_id: str, grant_url: str = None) -> Tuple[str, List[Tuple[str, str]]]:
        """
        Researches the grant application format using Google Search Grounding.
        Also attempts to find and download application format files.
        
        Args:
            grant_name: Name of the grant to research
            user_id: User ID for file organization
            grant_url: Optional URL of the grant page (from Observer)
            
        Returns:
            Tuple of (format_info, downloaded_files)
            - format_info: Application format information text
            - downloaded_files: List of (file_path, filename) tuples
        """
        import logging
        from google.genai.types import GenerateContentConfig, Tool, GoogleSearch
        
        logging.info(f"[DRAFTER] Researching format for: {grant_name}, URL: {grant_url}")
        
        # If we have a URL from Observer, try Playwright scraping first
        if grant_url and grant_url != 'N/A':
            try:
                logging.info(f"[DRAFTER] Using provided URL for direct scraping: {grant_url}")
                playwright_files = self._scrape_url_for_files(grant_url, user_id)
                if playwright_files:
                    logging.info(f"[DRAFTER] Found {len(playwright_files)} files from provided URL")
                    format_info = f"""
## ç”³è«‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæƒ…å ±

å…¬å¼ãƒšãƒ¼ã‚¸ ({grant_url}) ã‹ã‚‰ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¾ã—ãŸã€‚

è©³ç´°ãªç”³è«‹æ–¹æ³•ã¯æ·»ä»˜ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã”ç¢ºèªãã ã•ã„ã€‚
"""
                    return (format_info, playwright_files)
            except Exception as e:
                logging.warning(f"[DRAFTER] Direct URL scraping failed: {e}")
        
        research_prompt = f"""
ä»¥ä¸‹ã®åŠ©æˆé‡‘ã®ç”³è«‹æ›¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆè³ªå•é …ç›®ãƒ»è¨˜å…¥æ¬„ï¼‰ã‚’èª¿æŸ»ã—ã¦ãã ã•ã„ã€‚

åŠ©æˆé‡‘å: {grant_name}

èª¿æŸ»ã™ã¹ãå†…å®¹:
1. ç”³è«‹æ›¸ã®è³ªå•é …ç›®ï¼ˆä¾‹ï¼šå›£ä½“æ¦‚è¦ã€äº‹æ¥­è¨ˆç”»ã€äºˆç®—ãªã©ï¼‰
2. å„é …ç›®ã®æ–‡å­—æ•°åˆ¶é™ã‚„è¨˜å…¥ä¾‹
3. å¯©æŸ»ã®ãƒã‚¤ãƒ³ãƒˆãƒ»è©•ä¾¡åŸºæº–
4. å¿…è¦ãªæ·»ä»˜æ›¸é¡
5. **é‡è¦**: ç”³è«‹æ›¸ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆPDFã€Wordã€Excelãªã©ï¼‰ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URLãŒã‚ã‚Œã°ç‰¹å®šã—ã¦ãã ã•ã„

å‡ºåŠ›å½¢å¼:
## ç”³è«‹æ›¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

### è³ªå•é …ç›®
1. [é …ç›®å] ï¼ˆæ–‡å­—æ•°åˆ¶é™ãŒã‚ã‚Œã°è¨˜è¼‰ï¼‰
2. [é …ç›®å] ï¼ˆæ–‡å­—æ•°åˆ¶é™ãŒã‚ã‚Œã°è¨˜è¼‰ï¼‰
...

### å¯©æŸ»ãƒã‚¤ãƒ³ãƒˆ
- [ãƒã‚¤ãƒ³ãƒˆ1]
- [ãƒã‚¤ãƒ³ãƒˆ2]

### å¿…è¦æ›¸é¡
- [æ›¸é¡1]
- [æ›¸é¡2]

### ç”³è«‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«
- URL: [ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰URL]ï¼ˆè¦‹ã¤ã‹ã£ãŸå ´åˆã®ã¿è¨˜è¼‰ï¼‰
- ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: [PDF/Word/Excelç­‰]

â€»è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯ä¸€èˆ¬çš„ãªåŠ©æˆé‡‘ç”³è«‹æ›¸ã®å½¢å¼ã‚’æƒ³å®šã—ã¦ãã ã•ã„ã€‚
"""
        
        try:
            # Use Google Search Grounding for format research
            google_search_tool = Tool(google_search=GoogleSearch())
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=research_prompt,
                config=GenerateContentConfig(
                    tools=[google_search_tool],
                    temperature=0.3
                )
            )
            
            
            format_info = response.text
            logging.info(f"[DRAFTER] Format research completed, length: {len(format_info)} chars")
            
            # 1. Extract file URLs from the text response
            import re
            url_pattern = r'https?://[^\s<>"\)]+\.(?:pdf|doc|docx|xls|xlsx|zip)'
            found_urls = set(re.findall(url_pattern, format_info, re.IGNORECASE))
            
            # 2. Extract page URLs from Grounding Metadata and deep search
            try:
                if response.candidates and response.candidates[0].grounding_metadata:
                    metadata = response.candidates[0].grounding_metadata
                    if hasattr(metadata, 'grounding_chunks') and metadata.grounding_chunks:
                        for chunk in metadata.grounding_chunks:
                            if hasattr(chunk, 'web') and chunk.web and chunk.web.uri:
                                page_url = chunk.web.uri
                                
                                # Resolve redirect if needed (simple version)
                                if 'grounding-api-redirect' in page_url:
                                    try:
                                        import requests
                                        res = requests.head(page_url, allow_redirects=True, timeout=5)
                                        page_url = res.url
                                    except:
                                        pass
                                
                                logging.info(f"[DRAFTER] Deep searching page for files: {page_url}")
                                page_files = self.file_downloader.find_files_in_page(page_url)
                                found_urls.update(page_files)
            except Exception as e:
                logging.error(f"[DRAFTER] Error in deep search: {e}")
            
            downloaded_files = []
            failed_urls = []
            
            if found_urls:
                # Convert set back to list and sort to prioritize generic names? No, just list.
                url_list = list(found_urls)
                logging.info(f"[DRAFTER] Found {len(url_list)} potential format file URLs")
                
                # Filter out likely irrelevant files (images, common assets) if extension check failed
                # But regex ensures extension is valid.
                
                for url in url_list[:5]:  # Limit to first 5 URLs (increased from 3)
                    logging.info(f"[DRAFTER] Attempting to download: {url}")
                    result = self.file_downloader.download_file(url, user_id)
                    if result:
                        downloaded_files.append(result)
                        logging.info(f"[DRAFTER] Successfully downloaded: {result[1]}")
                    else:
                        failed_urls.append(url)
                        logging.warning(f"[DRAFTER] Failed to download: {url}")
                
                # Add download summary to format_info
                if downloaded_files or failed_urls:
                    summary = "\n\n---\n## ğŸ“ ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰çµæœ\n\n"
                    if downloaded_files:
                        summary += f"âœ… **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ**: {len(downloaded_files)}ä»¶\n"
                        for file_path, filename in downloaded_files:
                            summary += f"  - {filename}\n"
                    if failed_urls:
                        summary += f"\nâš ï¸ **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—**: {len(failed_urls)}ä»¶\n"
                        summary += "  ï¼ˆURLãŒç„¡åŠ¹ã€ã¾ãŸã¯ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸï¼‰\n"
                    format_info += summary
            else:
                logging.info("[DRAFTER] No format file URLs found in search results, trying Playwright deep search...")
                
                # Fallback: Use Playwright for deep search
                try:
                    playwright_files = self._run_playwright_deep_search(grant_name, user_id)
                    if playwright_files:
                        downloaded_files.extend(playwright_files)
                        summary = "\n\n---\n## ğŸ“ Playwrightæ·±æ˜ã‚Šæ¤œç´¢ã®çµæœ\n\n"
                        summary += f"âœ… **ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸ**: {len(playwright_files)}ä»¶\n"
                        for file_path, filename in playwright_files:
                            summary += f"  - {filename}\n"
                        format_info += summary
                    else:
                        logging.info("[DRAFTER] Playwright deep search also found no files")
                except Exception as pw_error:
                    logging.warning(f"[DRAFTER] Playwright deep search failed: {pw_error}")
            
            return (format_info, downloaded_files)
            
        except Exception as e:
            logging.error(f"[DRAFTER] Format research failed: {e}")
            # Return generic format as fallback
            return ("""
## ç”³è«‹æ›¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆä¸€èˆ¬çš„ãªå½¢å¼ï¼‰

### è³ªå•é …ç›®
1. å›£ä½“æ¦‚è¦ï¼ˆ400å­—ç¨‹åº¦ï¼‰
2. äº‹æ¥­ã®ç›®çš„ã¨èƒŒæ™¯ï¼ˆ600å­—ç¨‹åº¦ï¼‰
3. å…·ä½“çš„ãªæ´»å‹•è¨ˆç”»
4. æœŸå¾…ã•ã‚Œã‚‹æˆæœãƒ»åŠ¹æœ
5. äºˆç®—è¨ˆç”»
6. ä»Šå¾Œã®å±•æœ›

### å¯©æŸ»ãƒã‚¤ãƒ³ãƒˆ
- ç¤¾ä¼šçš„æ„ç¾©ã¨å¿…è¦æ€§
- å®Ÿç¾å¯èƒ½æ€§
- å›£ä½“ã®å®Ÿç¸¾ã¨ä¿¡é ¼æ€§
- è²»ç”¨å¯¾åŠ¹æœ
""", [])

    def _analyze_application_format(
        self, 
        format_files: List[Tuple[str, str]], 
        grant_name: str
    ) -> str:
        """
        ç”³è«‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å†…å®¹ã‚’Gemini 3.0 Proã§è§£æã—ã€
        è³ªå•é …ç›®ãƒ»è¨˜å…¥æ¬„ãƒ»æ–‡å­—æ•°åˆ¶é™ãªã©ã‚’æŠ½å‡ºã™ã‚‹ã€‚
        
        Args:
            format_files: ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒªã‚¹ãƒˆ[(file_path, filename), ...]
            grant_name: åŠ©æˆé‡‘å
            
        Returns:
            è§£æçµæœã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆè³ªå•é …ç›®ã€æ–‡å­—æ•°åˆ¶é™ã€è¨˜å…¥ã®ãƒã‚¤ãƒ³ãƒˆãªã©ï¼‰
        """
        if not format_files:
            logging.info("[DRAFTER] No format files to analyze")
            return ""
        
        logging.info(f"[DRAFTER] Analyzing {len(format_files)} format files with Gemini")
        
        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’åé›†
        file_contents_text = ""
        analyzed_count = 0
        
        for file_path, filename in format_files[:5]:  # æœ€å¤§5ãƒ•ã‚¡ã‚¤ãƒ«ã¾ã§
            try:
                file_ext = filename.lower().split('.')[-1] if '.' in filename else ''
                
                # PDFãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
                if file_ext == 'pdf':
                    content = self._extract_pdf_content(file_path)
                    if content:
                        file_contents_text += f"\n\n---\n### ãƒ•ã‚¡ã‚¤ãƒ«: {filename}\n{content[:8000]}\n"
                        analyzed_count += 1
                        
                # Word/Excelãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
                elif file_ext in ['doc', 'docx', 'xls', 'xlsx']:
                    content = self._extract_office_content(file_path, file_ext)
                    if content:
                        file_contents_text += f"\n\n---\n### ãƒ•ã‚¡ã‚¤ãƒ«: {filename}\n{content[:8000]}\n"
                        analyzed_count += 1
                        
                # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
                elif file_ext in ['txt', 'text']:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()[:8000]
                        file_contents_text += f"\n\n---\n### ãƒ•ã‚¡ã‚¤ãƒ«: {filename}\n{content}\n"
                        analyzed_count += 1
                        
            except Exception as e:
                logging.warning(f"[DRAFTER] Error reading file {filename}: {e}")
                continue
        
        if not file_contents_text or analyzed_count == 0:
            logging.info("[DRAFTER] Could not extract content from any files")
            return ""
        
        logging.info(f"[DRAFTER] Successfully extracted content from {analyzed_count} files")
        
        # Gemini 3.0 Proã§ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’è§£æ
        try:
            format_analyzer_prompt = self.config.get("system_prompts", {}).get("format_analyzer", "")
            if not format_analyzer_prompt:
                logging.warning("[DRAFTER] format_analyzer prompt not found in config")
                return ""
            
            # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã‚’åŸ‹ã‚è¾¼ã¿
            full_prompt = format_analyzer_prompt.replace("{file_contents}", file_contents_text)
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=full_prompt
            )
            
            analysis_result = response.text
            logging.info(f"[DRAFTER] Format analysis completed, length: {len(analysis_result)} chars")
            
            return analysis_result
            
        except Exception as e:
            logging.error(f"[DRAFTER] Format analysis failed: {e}")
            return ""
    
    def _extract_pdf_content(self, file_path: str) -> str:
        """PDFãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹"""
        try:
            import fitz  # PyMuPDF
            
            doc = fitz.open(file_path)
            text_content = ""
            
            for page_num in range(min(doc.page_count, 10)):  # æœ€å¤§10ãƒšãƒ¼ã‚¸ã¾ã§
                page = doc[page_num]
                text_content += page.get_text() + "\n"
            
            doc.close()
            return text_content.strip()
            
        except ImportError:
            logging.warning("[DRAFTER] PyMuPDF (fitz) not installed, skipping PDF extraction")
            return ""
        except Exception as e:
            logging.warning(f"[DRAFTER] PDF extraction failed: {e}")
            return ""
    
    def _extract_office_content(self, file_path: str, file_ext: str) -> str:
        """Word/Excelãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹"""
        try:
            if file_ext in ['doc', 'docx']:
                from docx import Document
                doc = Document(file_path)
                return "\n".join([para.text for para in doc.paragraphs])
                
            elif file_ext in ['xls', 'xlsx']:
                import openpyxl
                wb = openpyxl.load_workbook(file_path, read_only=True)
                text_content = ""
                
                for sheet in wb.worksheets[:3]:  # æœ€å¤§3ã‚·ãƒ¼ãƒˆã¾ã§
                    for row in sheet.iter_rows(max_row=100):  # æœ€å¤§100è¡Œã¾ã§
                        row_text = " | ".join([str(cell.value) if cell.value else "" for cell in row])
                        if row_text.strip():
                            text_content += row_text + "\n"
                
                return text_content.strip()
                
        except ImportError as e:
            logging.warning(f"[DRAFTER] Office library not installed: {e}")
            return ""
        except Exception as e:
            logging.warning(f"[DRAFTER] Office file extraction failed: {e}")
            return ""

    def _scrape_url_for_files(self, url: str, user_id: str) -> List[Tuple[str, str]]:

        """
        Scrape a specific URL for format files using Playwright.
        This is used when we have a verified URL from Observer.
        """
        try:
            import nest_asyncio
            nest_asyncio.apply()
            return asyncio.run(self._async_scrape_url_for_files(url, user_id))
        except Exception as e:
            logging.error(f"[DRAFTER] _scrape_url_for_files error: {e}")
            return []
    
    async def _async_scrape_url_for_files(self, url: str, user_id: str) -> List[Tuple[str, str]]:
        """
        Async method to scrape a URL for format files.
        """
        downloaded_files = []
        
        try:
            # Use page scraper to get grant info and files
            grant_info = await self.page_scraper.find_grant_info(url, "")
            
            if not grant_info.get('accessible'):
                logging.warning(f"[DRAFTER] Page not accessible: {url}")
                return []
            
            format_files = grant_info.get('format_files', [])
            logging.info(f"[DRAFTER] Found {len(format_files)} format files on page")
            
            # Download files
            for file_info in format_files[:5]:
                file_url = file_info.get('url')
                if not file_url:
                    continue
                
                logging.info(f"[DRAFTER] Downloading: {file_url}")
                result = self.file_downloader.download_file(file_url, user_id)
                if result:
                    downloaded_files.append(result)
                    logging.info(f"[DRAFTER] Downloaded: {result[1]}")
                    
        except Exception as e:
            logging.error(f"[DRAFTER] Async scrape error: {e}")
        
        return downloaded_files

    def _run_playwright_deep_search(self, grant_name: str, user_id: str) -> List[Tuple[str, str]]:
        """
        Run Playwright-based deep search for format files.
        Uses nest_asyncio to allow running async code within Discord.py's event loop.
        """
        try:
            import nest_asyncio
            nest_asyncio.apply()
            
            # Extract organization name for targeted search
            from src.logic.grant_validator import GrantValidator
            validator = GrantValidator()
            org_name = validator.extract_organization_name(grant_name)
            
            if not org_name:
                logging.info("[DRAFTER] Could not extract organization name for Playwright search")
                return []
            
            # Build search URL (use Google to find organization's grant page)
            search_query = f"{org_name} åŠ©æˆé‡‘ ç”³è«‹æ›¸ æ§˜å¼"
            search_url = f"https://www.google.com/search?q={search_query.replace(' ', '+')}"
            
            logging.info(f"[DRAFTER] Playwright deep search for: {org_name}")
            
            # Now we can safely run asyncio.run() within the existing event loop
            return asyncio.run(self._async_playwright_deep_search(search_url, grant_name, user_id))
            
        except Exception as e:
            logging.error(f"[DRAFTER] Playwright deep search error: {e}")
            return []
    
    async def _async_playwright_deep_search(
        self, 
        start_url: str, 
        grant_name: str, 
        user_id: str
    ) -> List[Tuple[str, str]]:
        """
        Async Playwright deep search for format files.
        """
        downloaded_files = []
        
        try:
            # Use deep search to find format files
            format_files = await self.page_scraper.deep_search_format_files(start_url, max_depth=2)
            
            if not format_files:
                logging.info("[DRAFTER] Playwright found no format files")
                return []
            
            logging.info(f"[DRAFTER] Playwright found {len(format_files)} potential files")
            
            # Download top-scored files
            for file_info in format_files[:5]:
                file_url = file_info.get('url')
                if not file_url:
                    continue
                
                logging.info(f"[DRAFTER] Downloading: {file_url}")
                result = self.file_downloader.download_file(file_url, user_id)
                if result:
                    downloaded_files.append(result)
                    logging.info(f"[DRAFTER] Downloaded: {result[1]}")
                    
        except Exception as e:
            logging.error(f"[DRAFTER] Async deep search error: {e}")
        
        return downloaded_files

    def create_draft(self, user_id: str, grant_info: str) -> tuple[str, str, str, List[Tuple[str, str]]]:
        """
        Generates a grant application draft based on researched format with progress notifications.
        
        Returns:
            tuple: (message, draft_content, filename, format_files)
            - format_files: List of (file_path, filename) tuples for downloaded files
        """
        import logging
        from src.utils.progress_notifier import get_progress_notifier, ProgressStage
        
        logging.info(f"[DRAFTER] create_draft started for user: {user_id}")
        notifier = get_progress_notifier()
        
        pm = ProfileManager(user_id=user_id)
        profile = pm.get_profile_context()
        
        logging.info(f"[DRAFTER] Profile loaded, length: {len(profile)} chars")
        
        # Extract grant name and URL from grant_info
        grant_name = grant_info.strip()
        grant_url = None
        
        # Try to extract URL from grant_info
        import re
        url_match = re.search(r'URL:\s*(https?://[^\s]+)', grant_info)
        if url_match:
            grant_url = url_match.group(1).strip()
            logging.info(f"[DRAFTER] Extracted URL from grant_info: {grant_url}")
        
        # Try to extract just the grant name if it contains other info
        name_match = re.search(r'åŠ©æˆé‡‘å:\s*(.+?)(?:\n|$)', grant_info)
        if name_match:
            grant_name = name_match.group(1).strip()
        elif "åŠ©æˆ" in grant_name:
            # Find the grant name pattern
            match = re.search(r'[^\s]+åŠ©æˆ[^\s]*', grant_name)
            if match:
                grant_name = match.group(0)
        
        # Create display name (max 20 chars) for notifications
        grant_display_name = grant_name[:20] + "..." if len(grant_name) > 20 else grant_name
        
        logging.info(f"[DRAFTER] Grant name: {grant_name}, URL: {grant_url}")
        
        # Start notification
        notifier.notify_sync(
            ProgressStage.STARTING,
            f"âœ¨ [{grant_display_name}] ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆã‚’é–‹å§‹ã—ã¾ã™..."
        )
        
        # Step 1: Research the application format (prioritize URL if available)
        logging.info(f"[DRAFTER] Step 1: Researching format for '{grant_name}'")
        format_info, format_files = self._research_grant_format(grant_name, user_id, grant_url=grant_url)
        
        # Step 2: Analyze downloaded format files with Gemini 3.0 Pro
        format_analysis = ""
        if format_files:
            logging.info(f"[DRAFTER] Step 2: Analyzing {len(format_files)} format files with Gemini")
            notifier.notify_sync(
                ProgressStage.PROCESSING,
                f"ğŸ“‹ [{grant_display_name}] ç”³è«‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’è§£æä¸­..."
            )
            format_analysis = self._analyze_application_format(format_files, grant_name)
            if format_analysis:
                logging.info(f"[DRAFTER] Format analysis completed, length: {len(format_analysis)} chars")
            else:
                logging.info("[DRAFTER] Format analysis returned empty, using format_info only")
        
        # Step 3: Generate draft based on format and analysis
        logging.info(f"[DRAFTER] Step 3: Generating format-aware draft")
        
        notifier.notify_sync(
            ProgressStage.PROCESSING,
            f"ğŸ“ [{grant_display_name}] ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ç”Ÿæˆä¸­..."
        )
        
        # Combine format_info and format_analysis for comprehensive context
        combined_format_info = format_info
        if format_analysis:
            combined_format_info += f"\n\n---\n\n# ç”³è«‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè©³ç´°è§£æçµæœ\n{format_analysis}"
        
        full_prompt = f"""
{self.system_prompt}

# Soul Profileï¼ˆé­‚ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
{profile}

# å¯¾è±¡åŠ©æˆé‡‘
{grant_info}

# ç”³è«‹æ›¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæƒ…å ±
{combined_format_info}

# ã‚¿ã‚¹ã‚¯
ä¸Šè¨˜ã®ç”³è«‹æ›¸ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆç‰¹ã«ã€Œç”³è«‹ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆè©³ç´°è§£æçµæœã€ã®è³ªå•é …ç›®ï¼‰ã«å¾“ã£ã¦ã€å„è³ªå•é …ç›®ã«å¯¾ã™ã‚‹å›ç­”ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

**é‡è¦ãªæŒ‡ç¤º:**
1. è§£æçµæœã®è³ªå•é …ç›®ä¸€è¦§ã«è¨˜è¼‰ã•ã‚ŒãŸé …ç›®ã”ã¨ã«è¦‹å‡ºã—ã‚’ä»˜ã‘ã¦å›ç­”ã‚’ä½œæˆ
2. æ–‡å­—æ•°åˆ¶é™ãŒæ˜è¨˜ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ãã‚Œã«åã¾ã‚‹ã‚ˆã†ã«èª¿æ•´
3. Soul Profileã®æƒ…å ±ã‚’æœ€å¤§é™æ´»ç”¨
4. å„å›ç­”ã®å¾Œã«ç°¡å˜ãªğŸ“è¨˜å…¥ã®ãƒã‚¤ãƒ³ãƒˆã‚’è¿½è¨˜
5. å¯©æŸ»ã§é‡è¦–ã•ã‚Œã‚‹ç‚¹ã‚’æ„è­˜ã—ã¦å›ç­”ã‚’ä½œæˆ

**å‡ºåŠ›å½¢å¼:**
# [åŠ©æˆé‡‘å] ç”³è«‹æ›¸ãƒ‰ãƒ©ãƒ•ãƒˆ

## 1. [è³ªå•é …ç›®1]
[å›ç­”å†…å®¹]
ğŸ“ ãƒã‚¤ãƒ³ãƒˆ: [ã“ã®é …ç›®ã§å¼·èª¿ã™ã¹ãç‚¹]

## 2. [è³ªå•é …ç›®2]
[å›ç­”å†…å®¹]
ğŸ“ ãƒã‚¤ãƒ³ãƒˆ: [ã“ã®é …ç›®ã§å¼·èª¿ã™ã¹ãç‚¹]

...

---
## ğŸ“‹ å…¨ä½“ã®è€ƒæ…®ç‚¹
[ç”³è«‹å…¨ä½“ã§æ°—ã‚’ã¤ã‘ã‚‹ã¹ãç‚¹]

## ğŸŒŸ ã‚¢ãƒ”ãƒ¼ãƒ«ãƒã‚¤ãƒ³ãƒˆ
[ç‰¹ã«å¼·èª¿ã™ã¹ãå›£ä½“ã®å¼·ã¿]

## âš ï¸ æ‡¸å¿µç‚¹ãƒ»æ”¹å–„ææ¡ˆ
[ç”³è«‹ã§å¼±ããªã‚Šãã†ãªç‚¹ã¨å¯¾ç­–]
"""
        try:
            logging.info(f"[DRAFTER] Calling Gemini model: {self.model_name}")
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=full_prompt
            )
            draft_content = response.text
            logging.info(f"[DRAFTER] Draft generated, length: {len(draft_content)} chars")
            
            # Extract a title (first line or generic)
            lines = draft_content.split('\n')
            title = "Grant_Draft"
            if lines and lines[0].startswith('# '):
                 title = lines[0].replace('# ', '').strip()
            
            logging.info(f"[DRAFTER] Title: {title}")
            
            notifier.notify_sync(
                ProgressStage.PROCESSING,
                f"ğŸ’¾ [{grant_display_name}] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä¿å­˜ä¸­..."
            )
            
            file_path = self.docs_tool.create_document(title, draft_content, user_id=user_id)
            logging.info(f"[DRAFTER] Document saved: {file_path}")
            
            # Extract filename from path
            import os
            if 'gs://' in file_path:
                # GCS path: gs://bucket/drafts/user_id/filename.md
                filename = file_path.split('/')[-1]
            elif 'Google Doc' in file_path:
                # Google Docs: extract from message
                filename = f"{title}.md"
            else:
                # Local path
                filename = os.path.basename(file_path)
            
            logging.info(f"[DRAFTER] Filename: {filename}")
            
            message = f"ãƒ‰ãƒ©ãƒ•ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸ: {file_path}"
            
            # Completion notification
            notifier.notify_sync(
                ProgressStage.COMPLETED,
                f"âœ… [{grant_display_name}] ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆå®Œäº†ï¼"
            )
            
            logging.info(f"[DRAFTER] create_draft completed successfully")
            return (message, draft_content, filename, format_files)
            
        except Exception as e:
            logging.error(f"[DRAFTER] Error in create_draft: {e}", exc_info=True)
            
            # Error notification
            notifier.notify_sync(
                ProgressStage.ERROR,
                f"âš ï¸ [{grant_display_name}] ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {str(e)[:30]}..."
            )
            
            error_msg = f"ãƒ‰ãƒ©ãƒ•ãƒˆä½œæˆã‚¨ãƒ©ãƒ¼: {e}"
            return (error_msg, "", "", [])


    def list_drafts(self, user_id: str) -> str:
        """
        Lists all drafts for a user.
        
        Args:
            user_id: User ID
            
        Returns:
            Formatted list of drafts or message if none found
        """
        try:
            drafts = self.docs_tool.list_drafts(user_id)
            
            if not drafts:
                return "ã¾ã ãƒ‰ãƒ©ãƒ•ãƒˆãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã€ŒåŠ©æˆé‡‘ç”³è«‹æ›¸ã‚’æ›¸ã„ã¦ã€ã¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚"
            
            result = f"ğŸ“„ **ä¿å­˜æ¸ˆã¿ãƒ‰ãƒ©ãƒ•ãƒˆä¸€è¦§** ({len(drafts)}ä»¶)\n\n"
            for i, filename in enumerate(drafts, 1):
                result += f"{i}. `{filename}`\n"
            
            result += "\nğŸ’¡ ç‰¹å®šã®ãƒ‰ãƒ©ãƒ•ãƒˆã‚’è¦‹ã‚‹ã«ã¯ã€Œ[ãƒ•ã‚¡ã‚¤ãƒ«å]ã‚’è¦‹ã›ã¦ã€ã¾ãŸã¯ã€Œæœ€æ–°ã®ãƒ‰ãƒ©ãƒ•ãƒˆã‚’è¦‹ã›ã¦ã€ã¨é€ä¿¡ã—ã¦ãã ã•ã„ã€‚"
            
            return result
            
        except Exception as e:
            return f"ãƒ‰ãƒ©ãƒ•ãƒˆä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}"

    def clear_drafts(self, user_id: str) -> str:
        """
        Clears all drafts for a user.
        
        Args:
            user_id: User ID
            
        Returns:
            Success message
        """
        try:
            return self.docs_tool.clear_drafts(user_id)
        except Exception as e:
            return f"ãƒ‰ãƒ©ãƒ•ãƒˆã‚¯ãƒªã‚¢ã‚¨ãƒ©ãƒ¼: {e}"

    def get_latest_draft(self, user_id: str) -> tuple[str, Optional[str]]:
        """
        Gets the latest draft for a user.
        
        Args:
            user_id: User ID
            
        Returns:
            Tuple of (message, content). If content is present, it should be sent as attachment.
        """
        try:
            drafts = self.docs_tool.list_drafts(user_id)
            
            if not drafts:
                return ("ã¾ã ãƒ‰ãƒ©ãƒ•ãƒˆãŒä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚", None)
            
            # Sort by filename (which includes timestamp)
            latest_draft = sorted(drafts)[-1]
            content = self.docs_tool.get_draft(user_id, latest_draft)
            
            if not content:
                return (f"ãƒ‰ãƒ©ãƒ•ãƒˆ '{latest_draft}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚", None)
            
            message = f"ğŸ“„ **æœ€æ–°ã®ãƒ‰ãƒ©ãƒ•ãƒˆ**: `{latest_draft}`\n\n"
            
            # If content is short, include it in message
            if len(content) <= 1800:
                message += f"```markdown\n{content}\n```"
                return (message, None)
            else:
                # Return content for file attachment
                message += "ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦é€ä¿¡ã—ã¾ã™ï¼‰"
                return (message, content)
                
        except Exception as e:
            return (f"æœ€æ–°ãƒ‰ãƒ©ãƒ•ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}", None)

    def get_draft(self, user_id: str, filename: str) -> tuple[str, Optional[str]]:
        """
        Gets a specific draft by filename.
        
        Args:
            user_id: User ID
            filename: Draft filename
            
        Returns:
            Tuple of (message, content). If content is present, it should be sent as attachment.
        """
        try:
            content = self.docs_tool.get_draft(user_id, filename)
            
            if not content:
                # Try fuzzy match
                drafts = self.docs_tool.list_drafts(user_id)
                matches = [d for d in drafts if filename.lower() in d.lower()]
                
                if matches:
                    if len(matches) == 1:
                        # Use the matched file
                        filename = matches[0]
                        content = self.docs_tool.get_draft(user_id, filename)
                    else:
                        suggestion = "\n\nå€™è£œ:\n" + "\n".join([f"- {m}" for m in matches])
                        return (f"ãƒ•ã‚¡ã‚¤ãƒ«åãŒæ›–æ˜§ã§ã™ã€‚{suggestion}", None)
                else:
                    return (f"ãƒ‰ãƒ©ãƒ•ãƒˆ '{filename}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã€Œãƒ‰ãƒ©ãƒ•ãƒˆä¸€è¦§ã€ã§ç¢ºèªã—ã¦ãã ã•ã„ã€‚", None)
            
            message = f"ğŸ“„ **ãƒ‰ãƒ©ãƒ•ãƒˆ**: `{filename}`\n\n"
            
            # If content is short, include it in message
            if len(content) <= 1800:
                message += f"```markdown\n{content}\n```"
                return (message, None)
            else:
                # Return content for file attachment
                message += "ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦é€ä¿¡ã—ã¾ã™ï¼‰"
                return (message, content)
                
        except Exception as e:
            return (f"ãƒ‰ãƒ©ãƒ•ãƒˆå–å¾—ã‚¨ãƒ©ãƒ¼: {e}", None)

